{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Intro","text":"<p><code>any-llm</code> is a Python library providing a single interface to different llm providers.</p>"},{"location":"#demo","title":"Demo","text":"<p>Try <code>any-llm</code> in action with our interactive chat demo that showcases streaming completions and provider switching:</p> <p>\ud83d\udcc2 Run the Demo</p> <p>The demo features real-time streaming responses, multiple provider support, and collapsible \"thinking\" content display.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Refer to the Quickstart for instructions on installation and usage.</p>"},{"location":"#api-documentation","title":"API Documentation","text":"<p><code>any-llm</code> provides two main interfaces:</p> <p>Direct API Functions (recommended for simple use cases): - completion - Chat completions with any provider - embedding - Text embeddings - responses - OpenAI-style Responses API</p> <p>AnyLLM Class (recommended for advanced use cases): - Provider API - Lower-level provider interface with metadata access and reusability</p>"},{"location":"#error-handling","title":"Error Handling","text":"<p><code>any-llm</code> provides custom exceptions to indicate common errors like missing API keys and parameters that are unsupported by a specific provider.</p> <p>For more details on exceptions, see the exceptions API documentation.</p>"},{"location":"#for-ai-systems","title":"For AI Systems","text":"<p>This documentation is available in two AI-friendly formats:</p> <ul> <li>llms.txt - A structured overview with curated links to key documentation sections</li> <li>llms-full.txt - Complete documentation content concatenated into a single file</li> </ul>"},{"location":"providers/","title":"Supported Providers","text":"<p><code>any-llm</code> supports the below providers. In order to discover information about what models are supported by a provider as well as what features the provider supports for each model, refer to the provider documentation.</p> <p>Provider source code can be found in the <code>src/any_llm/providers/</code> directory of the repository.</p> <p>Legend</p> <ul> <li>Reasoning (Completions): Provider can return reasoning traces alongside the assistant message via the completions and/or streaming endpoints. This does not indicate whether the provider offers separate \"reasoning models\".See this</li> <li>Streaming (Completions): Provider can stream completion results back as an iterator. discussion for more information.</li> <li>Image (Completions): Provider supports passing an <code>image_data</code> parameter for vision capabilities, as defined by the OpenAI spec here.</li> <li>Responses API: Provider supports the Responses API variant for text generation.  See this to follow along with our implementation effort.</li> <li>List Models API: Provider supports listing available models programmatically via the <code>list_models()</code> function. This allows you to discover what models are available from the provider at runtime, which can be useful for dynamic model selection or validation.</li> </ul> ID Env Var Responses Completion Streaming(Completions) Reasoning(Completions) Image (Completions) Embedding List Models Batch <code>anthropic</code> ANTHROPIC_API_KEY \u274c \u2705 \u2705 \u2705 \u2705 \u274c \u2705 \u274c <code>azure</code> AZURE_API_KEY \u274c \u2705 \u2705 \u274c \u274c \u2705 \u274c \u274c <code>azureopenai</code> AZURE_OPENAI_API_KEY \u2705 \u2705 \u2705 \u274c \u2705 \u2705 \u2705 \u274c <code>bedrock</code> AWS_BEARER_TOKEN_BEDROCK \u274c \u2705 \u2705 \u2705 \u274c \u2705 \u2705 \u274c <code>cerebras</code> CEREBRAS_API_KEY \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>cohere</code> COHERE_API_KEY \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>databricks</code> DATABRICKS_TOKEN \u274c \u2705 \u2705 \u2705 \u274c \u2705 \u274c \u274c <code>deepseek</code> DEEPSEEK_API_KEY \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>fireworks</code> FIREWORKS_API_KEY \u2705 \u2705 \u2705 \u2705 \u2705 \u274c \u2705 \u274c <code>gateway</code> GATEWAY_API_KEY \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 <code>gemini</code> GEMINI_API_KEY/GOOGLE_API_KEY \u274c \u2705 \u2705 \u2705 \u274c \u2705 \u2705 \u274c <code>groq</code> GROQ_API_KEY \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>huggingface</code> HF_TOKEN \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>inception</code> INCEPTION_API_KEY \u274c \u2705 \u2705 \u274c \u274c \u274c \u2705 \u274c <code>llama</code> LLAMA_API_KEY \u274c \u2705 \u2705 \u274c \u274c \u274c \u2705 \u274c <code>llamacpp</code> None \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>llamafile</code> None \u274c \u2705 \u274c \u2705 \u274c \u274c \u2705 \u274c <code>lmstudio</code> LM_STUDIO_API_KEY \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>minimax</code> MINIMAX_API_KEY \u274c \u2705 \u2705 \u2705 \u274c \u274c \u274c \u274c <code>mistral</code> MISTRAL_API_KEY \u274c \u2705 \u2705 \u2705 \u274c \u2705 \u2705 \u274c <code>moonshot</code> MOONSHOT_API_KEY \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>nebius</code> NEBIUS_API_KEY \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>ollama</code> None \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>openai</code> OPENAI_API_KEY \u2705 \u2705 \u2705 \u274c \u2705 \u2705 \u2705 \u2705 <code>openrouter</code> OPENROUTER_API_KEY \u274c \u2705 \u2705 \u2705 \u2705 \u274c \u2705 \u274c <code>perplexity</code> PERPLEXITY_API_KEY \u274c \u2705 \u2705 \u274c \u2705 \u274c \u274c \u274c <code>portkey</code> PORTKEY_API_KEY \u274c \u2705 \u2705 \u2705 \u2705 \u274c \u2705 \u274c <code>sagemaker</code> AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY \u274c \u2705 \u2705 \u274c \u2705 \u2705 \u274c \u274c <code>sambanova</code> SAMBANOVA_API_KEY \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>together</code> TOGETHER_API_KEY \u274c \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c <code>vertexai</code> \u274c \u2705 \u2705 \u2705 \u274c \u2705 \u2705 \u274c <code>voyage</code> VOYAGE_API_KEY \u274c \u274c \u274c \u274c \u274c \u2705 \u274c \u274c <code>watsonx</code> WATSONX_API_KEY \u274c \u2705 \u2705 \u274c \u2705 \u274c \u2705 \u274c <code>xai</code> XAI_API_KEY \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>zai</code> ZAI_API_KEY \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#quickstart","title":"Quickstart","text":""},{"location":"quickstart/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or newer</li> <li>API_KEYS to access to whichever LLM you choose to use.</li> </ul>"},{"location":"quickstart/#installation","title":"Installation","text":""},{"location":"quickstart/#direct-usage","title":"Direct Usage","text":"<p>In your pip install, include the supported providers that you plan on using, or use the <code>all</code> option if you want to install support for all <code>any-llm</code> supported providers.</p> <pre><code>pip install any-llm-sdk[mistral]  # For Mistral provider\npip install any-llm-sdk[ollama]   # For Ollama provider\n# install multiple providers\npip install any-llm-sdk[mistral,ollama]\n# or install support for all providers\npip install any-llm-sdk[all]\n</code></pre>"},{"location":"quickstart/#library-integration","title":"Library Integration","text":"<p>If you're integrating <code>any-llm</code> into your own library that others will use, you only need to install the base package:</p> <pre><code>pip install any-llm-sdk\n</code></pre> <p>In this scenario, the end users of your library will be responsible for installing the appropriate provider dependencies when they want to use specific providers. <code>any-llm</code> is designed so that you'll only encounter exceptions at runtime if you try to use a provider without having the required dependencies installed.</p> <p>Those exceptions will clearly describe what needs to be installed to resolve the issue.</p> <p>Make sure you have the appropriate API key environment variable set for your provider. Alternatively, you could use the <code>api_key</code> parameter when making a completion call instead of setting an environment variable.</p> <pre><code>export MISTRAL_API_KEY=\"YOUR_KEY_HERE\"  # or OPENAI_API_KEY, etc\n</code></pre>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":"<p><code>any-llm</code> provides two main approaches for working with LLM providers, each optimized for different use cases:</p>"},{"location":"quickstart/#option-1-direct-api-functions","title":"Option 1: Direct API Functions","text":"<p><code>completion</code> and <code>acompletion</code> provide a unified interface across all providers - perfect for simple use cases and quick prototyping.</p> <p>Recommended approach: Use separate <code>provider</code> and <code>model</code> parameters:</p> <pre><code>import os\n\nfrom any_llm import completion\n\n# Make sure you have the appropriate environment variable set\nassert os.environ.get('MISTRAL_API_KEY')\n\n# Recommended: separate provider and model parameters\nresponse = completion(\n    model=\"mistral-small-latest\",\n    provider=\"mistral\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\nprint(response.choices[0].message.content)\n</code></pre> <p>Alternative syntax: You can also use the combined <code>provider:model</code> format:</p> <pre><code>response = completion(\n    model=\"mistral:mistral-small-latest\",  # &lt;provider_id&gt;:&lt;model_id&gt;\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre>"},{"location":"quickstart/#option-2-anyllm-class","title":"Option 2: AnyLLM Class","text":"<p>For advanced use cases that require provider reuse, metadata access, or more control over configuration:</p> <pre><code>import os\n\nfrom any_llm import AnyLLM\n\n# Make sure you have the appropriate environment variable set\nassert os.environ.get('MISTRAL_API_KEY')\n\nllm = AnyLLM.create(\"mistral\")\n\nresponse = llm.completion(\n    model=\"mistral-small-latest\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\nprint(response.choices[0].message.content)\n\nmetadata = llm.get_provider_metadata()\nprint(f\"Supports streaming: {metadata.streaming}\")\nprint(f\"Supports tools: {metadata.completion}\")\n</code></pre>"},{"location":"quickstart/#when-to-choose-which-approach","title":"When to Choose Which Approach","text":"<p>Use Direct API Functions (<code>completion</code>, <code>acompletion</code>) when:</p> <ul> <li>Making simple, one-off requests</li> <li>Prototyping or writing quick scripts</li> <li>You want the simplest possible interface</li> </ul> <p>Use Provider Class (<code>AnyLLM.create</code>) when:</p> <ul> <li>Building applications that make multiple requests with the same provider</li> <li>You want to avoid repeated provider instantiation overhead</li> </ul> <p>The provider_id should be specified according to the provider ids supported by any-llm. The <code>model_id</code> portion is passed directly to the provider internals: to understand what model ids are available for a provider, you will need to refer to the provider documentation or use our <code>list_models</code>  API if the provider supports that API.</p>"},{"location":"quickstart/#streaming","title":"Streaming","text":"<p>For the providers that support streaming, you can enable it by passing <code>stream=True</code>:</p> <pre><code>output = \"\"\nfor chunk in completion(\n    model=\"mistral-small-latest\",\n    provider=\"mistral\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    stream=True\n):\n    chunk_content = chunk.choices[0].delta.content or \"\"\n    print(chunk_content)\n    output += chunk_content\n</code></pre>"},{"location":"quickstart/#embeddings","title":"Embeddings","text":"<p><code>embedding</code> and <code>aembedding</code> allow you to create vector embeddings from text using the same unified interface across providers.</p> <p>Not all providers support embeddings - check the providers documentation to see which ones do.</p> <pre><code>from any_llm import embedding\n\nresult = embedding(\n    model=\"text-embedding-3-small\",\n    provider=\"openai\",\n    inputs=\"Hello, world!\" # can be either string or list of strings\n)\n\n# Access the embedding vector\nembedding_vector = result.data[0].embedding\nprint(f\"Embedding vector length: {len(embedding_vector)}\")\nprint(f\"Tokens used: {result.usage.total_tokens}\")\n</code></pre>"},{"location":"quickstart/#tools","title":"Tools","text":"<p><code>any-llm</code> supports tool calling for providers that support it. You can pass a list of tools where each tool is either:</p> <ol> <li>Python callable - Functions with proper docstrings and type annotations</li> <li>OpenAI Format tool dict - Already in OpenAI tool format</li> </ol> <pre><code>from any_llm import completion\n\ndef get_weather(location: str, unit: str = \"F\") -&gt; str:\n    \"\"\"Get weather information for a location.\n\n    Args:\n        location: The city or location to get weather for\n        unit: Temperature unit, either 'C' or 'F'\n    \"\"\"\n    return f\"Weather in {location} is sunny and 75{unit}!\"\n\nresponse = completion(\n    model=\"mistral-small-latest\",\n    provider=\"mistral\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Pittsburgh PA?\"}],\n    tools=[get_weather]\n)\n</code></pre> <p>any-llm automatically converts your Python functions to OpenAI tools format. Functions must have: - A docstring describing what the function does - Type annotations for all parameters - A return type annotation</p>"},{"location":"api/any_llm/","title":"AnyLLM","text":""},{"location":"api/any_llm/#anyllm","title":"AnyLLM","text":""},{"location":"api/any_llm/#any_llm.AnyLLM","title":"<code>any_llm.AnyLLM</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Provider for the LLM.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>class AnyLLM(ABC):\n    \"\"\"Provider for the LLM.\"\"\"\n\n    # === Provider-specific configuration (to be overridden by subclasses) ===\n    PROVIDER_NAME: str\n    \"\"\"Must match the name of the provider directory  (case sensitive)\"\"\"\n\n    PROVIDER_DOCUMENTATION_URL: str\n    \"\"\"Link to the provider's documentation\"\"\"\n\n    ENV_API_KEY_NAME: str\n    \"\"\"Environment variable name for the API key\"\"\"\n\n    # === Feature support flags (to be set by subclasses) ===\n    SUPPORTS_COMPLETION_STREAMING: bool\n    \"\"\"OpenAI Streaming Completion API\"\"\"\n\n    SUPPORTS_COMPLETION: bool\n    \"\"\"OpenAI Completion API\"\"\"\n\n    SUPPORTS_COMPLETION_REASONING: bool\n    \"\"\"Reasoning Content attached to Completion API Response\"\"\"\n\n    SUPPORTS_COMPLETION_IMAGE: bool\n    \"\"\"Image Support for Completion API\"\"\"\n\n    SUPPORTS_COMPLETION_PDF: bool\n    \"\"\"PDF Support for Completion API\"\"\"\n\n    SUPPORTS_EMBEDDING: bool\n    \"\"\"OpenAI Embedding API\"\"\"\n\n    SUPPORTS_RESPONSES: bool\n    \"\"\"OpenAI Responses API\"\"\"\n\n    SUPPORTS_LIST_MODELS: bool\n    \"\"\"OpenAI Models API\"\"\"\n\n    SUPPORTS_BATCH: bool\n    \"\"\"OpenAI Batch Completion API\"\"\"\n\n    API_BASE: str | None = None\n    \"\"\"This is used to set the API base for the provider.\n    It is not required but may prove useful for providers that have overridable api bases.\n    \"\"\"\n\n    # === Internal Flag Checks ===\n    MISSING_PACKAGES_ERROR: ImportError | None = None\n    \"\"\"Some providers use SDKs that are not installed by default.\n    This flag is used to check if the packages are installed before instantiating the provider.\n    \"\"\"\n\n    BUILT_IN_TOOLS: ClassVar[list[Any] | None] = None\n    \"\"\"Some providers have built-in tools that can be used as-is without conversion.\n    This should be a list of the allowed built-in tool instances.\n    For example, in `gemini` provider, this could include `google.genai.types.Tool`.\n    \"\"\"\n\n    def __init__(self, api_key: str | None = None, api_base: str | None = None, **kwargs: Any) -&gt; None:\n        self._verify_no_missing_packages()\n        self._init_client(\n            api_key=self._verify_and_set_api_key(api_key),\n            api_base=api_base,\n            **kwargs,\n        )\n\n    def _verify_no_missing_packages(self) -&gt; None:\n        if self.MISSING_PACKAGES_ERROR is not None:\n            msg = f\"{self.PROVIDER_NAME} required packages are not installed. Please install them with `pip install any-llm-sdk[{self.PROVIDER_NAME}]`\"\n            raise ImportError(msg) from self.MISSING_PACKAGES_ERROR\n\n    def _verify_and_set_api_key(self, api_key: str | None = None) -&gt; str | None:\n        # Standardized API key handling. Splitting into its own function so that providers\n        # can easily override this method if they don't want verification (for instance, LMStudio)\n        if not api_key:\n            api_key = os.getenv(self.ENV_API_KEY_NAME)\n\n        if not api_key:\n            raise MissingApiKeyError(self.PROVIDER_NAME, self.ENV_API_KEY_NAME)\n        return api_key\n\n    @classmethod\n    def create(\n        cls, provider: str | LLMProvider, api_key: str | None = None, api_base: str | None = None, **kwargs: Any\n    ) -&gt; AnyLLM:\n        \"\"\"Create a provider instance using the given provider name and config.\n\n        Args:\n            provider: The provider name (e.g., 'openai', 'anthropic')\n            api_key: API key for the provider\n            api_base: Base URL for the provider API\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            Provider instance for the specified provider\n\n        \"\"\"\n        return cls._create_provider(provider, api_key=api_key, api_base=api_base, **kwargs)\n\n    @classmethod\n    def _create_provider(\n        cls, provider_key: str | LLMProvider, api_key: str | None = None, api_base: str | None = None, **kwargs: Any\n    ) -&gt; AnyLLM:\n        \"\"\"Dynamically load and create an instance of a provider based on the naming convention.\"\"\"\n        provider_key = LLMProvider.from_string(provider_key).value\n\n        provider_class_name = f\"{provider_key.capitalize()}Provider\"\n        provider_module_name = f\"{provider_key}\"\n\n        module_path = f\"any_llm.providers.{provider_module_name}\"\n\n        try:\n            module = importlib.import_module(module_path)\n        except ImportError as e:\n            msg = f\"Could not import module {module_path}: {e!s}. Please ensure the provider is supported by doing AnyLLM.get_supported_providers()\"\n            raise ImportError(msg) from e\n\n        provider_class: type[AnyLLM] = getattr(module, provider_class_name)\n        return provider_class(api_key=api_key, api_base=api_base, **kwargs)\n\n    @classmethod\n    def get_provider_class(cls, provider_key: str | LLMProvider) -&gt; type[AnyLLM]:\n        \"\"\"Get the provider class without instantiating it.\n\n        Args:\n            provider_key: The provider key (e.g., 'anthropic', 'openai')\n\n        Returns:\n            The provider class\n\n        \"\"\"\n        provider_key = LLMProvider.from_string(provider_key).value\n\n        provider_class_name = f\"{provider_key.capitalize()}Provider\"\n        provider_module_name = f\"{provider_key}\"\n\n        module_path = f\"any_llm.providers.{provider_module_name}\"\n\n        try:\n            module = importlib.import_module(module_path)\n        except ImportError as e:\n            msg = f\"Could not import module {module_path}: {e!s}. Please ensure the provider is supported by doing AnyLLM.get_supported_providers()\"\n            raise ImportError(msg) from e\n\n        provider_class: type[AnyLLM] = getattr(module, provider_class_name)\n        return provider_class\n\n    @classmethod\n    def get_supported_providers(cls) -&gt; list[str]:\n        \"\"\"Get a list of supported provider keys.\"\"\"\n        return [provider.value for provider in LLMProvider]\n\n    @classmethod\n    def get_all_provider_metadata(cls) -&gt; list[ProviderMetadata]:\n        \"\"\"Get metadata for all supported providers.\n\n        Returns:\n            List of dictionaries containing provider metadata\n\n        \"\"\"\n        providers: list[ProviderMetadata] = []\n        for provider_key in cls.get_supported_providers():\n            provider_class = cls.get_provider_class(provider_key)\n            metadata = provider_class.get_provider_metadata()\n            providers.append(metadata)\n\n        # Sort providers by name\n        providers.sort(key=lambda x: x.name)\n        return providers\n\n    @classmethod\n    def get_provider_enum(cls, provider_key: str) -&gt; LLMProvider:\n        \"\"\"Convert a string provider key to a ProviderName enum.\"\"\"\n        try:\n            return LLMProvider(provider_key)\n        except ValueError as e:\n            supported = [provider.value for provider in LLMProvider]\n            raise UnsupportedProviderError(provider_key, supported) from e\n\n    @classmethod\n    def split_model_provider(cls, model: str) -&gt; tuple[LLMProvider, str]:\n        \"\"\"Extract the provider key from the model identifier.\n\n        Supports both new format 'provider:model' (e.g., 'mistral:mistral-small')\n        and legacy format 'provider/model' (e.g., 'mistral/mistral-small').\n\n        The legacy format will be deprecated in version 1.0.\n        \"\"\"\n        colon_index = model.find(\":\")\n        slash_index = model.find(\"/\")\n\n        # Determine which delimiter comes first\n        if colon_index != -1 and (slash_index == -1 or colon_index &lt; slash_index):\n            # The colon came first, so it's using the new syntax.\n            provider, model_name = model.split(\":\", 1)\n        elif slash_index != -1:\n            # Slash comes first, so it's the legacy syntax\n            warnings.warn(\n                f\"Model format 'provider/model' is deprecated and will be removed in version 1.0. \"\n                f\"Please use 'provider:model' format instead. Got: '{model}'\",\n                DeprecationWarning,\n                stacklevel=3,\n            )\n            provider, model_name = model.split(\"/\", 1)\n        else:\n            msg = f\"Invalid model format. Expected 'provider:model' or 'provider/model', got '{model}'\"\n            raise ValueError(msg)\n\n        if not provider or not model_name:\n            msg = f\"Invalid model format. Expected 'provider:model' or 'provider/model', got '{model}'\"\n            raise ValueError(msg)\n        return cls.get_provider_enum(provider), model_name\n\n    @staticmethod\n    @abstractmethod\n    def _convert_completion_params(params: CompletionParams, **kwargs: Any) -&gt; dict[str, Any]:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @staticmethod\n    @abstractmethod\n    def _convert_completion_response(response: Any) -&gt; ChatCompletion:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @staticmethod\n    @abstractmethod\n    def _convert_completion_chunk_response(response: Any, **kwargs: Any) -&gt; ChatCompletionChunk:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @staticmethod\n    @abstractmethod\n    def _convert_embedding_params(params: Any, **kwargs: Any) -&gt; dict[str, Any]:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @staticmethod\n    @abstractmethod\n    def _convert_embedding_response(response: Any) -&gt; CreateEmbeddingResponse:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @staticmethod\n    @abstractmethod\n    def _convert_list_models_response(response: Any) -&gt; Sequence[Model]:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @classmethod\n    def get_provider_metadata(cls) -&gt; ProviderMetadata:\n        \"\"\"Get provider metadata without requiring instantiation.\n\n        Returns:\n            Dictionary containing provider metadata including name, environment variable,\n            documentation URL, and class name.\n\n        \"\"\"\n        return ProviderMetadata(\n            name=cls.PROVIDER_NAME,\n            env_key=cls.ENV_API_KEY_NAME,\n            doc_url=cls.PROVIDER_DOCUMENTATION_URL,\n            streaming=cls.SUPPORTS_COMPLETION_STREAMING,\n            reasoning=cls.SUPPORTS_COMPLETION_REASONING,\n            completion=cls.SUPPORTS_COMPLETION,\n            image=cls.SUPPORTS_COMPLETION_IMAGE,\n            pdf=cls.SUPPORTS_COMPLETION_PDF,\n            embedding=cls.SUPPORTS_EMBEDDING,\n            responses=cls.SUPPORTS_RESPONSES,\n            list_models=cls.SUPPORTS_LIST_MODELS,\n            batch_completion=cls.SUPPORTS_BATCH,\n            class_name=cls.__name__,\n        )\n\n    @abstractmethod\n    def _init_client(self, api_key: str | None = None, api_base: str | None = None, **kwargs: Any) -&gt; None:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    def completion(\n        self,\n        **kwargs: Any,\n    ) -&gt; ChatCompletion | Iterator[ChatCompletionChunk]:\n        \"\"\"Create a chat completion synchronously.\n\n        See [AnyLLM.acompletion][any_llm.any_llm.AnyLLM.acompletion]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        response = run_async_in_sync(self.acompletion(**kwargs), allow_running_loop=allow_running_loop)\n        if isinstance(response, ChatCompletion):\n            return response\n\n        return async_iter_to_sync_iter(response)\n\n    async def acompletion(\n        self,\n        model: str,\n        messages: list[dict[str, Any] | ChatCompletionMessage],\n        *,\n        tools: list[dict[str, Any] | Callable[..., Any]] | Any | None = None,\n        tool_choice: str | dict[str, Any] | None = None,\n        temperature: float | None = None,\n        top_p: float | None = None,\n        max_tokens: int | None = None,\n        response_format: dict[str, Any] | type[BaseModel] | None = None,\n        stream: bool | None = None,\n        n: int | None = None,\n        stop: str | list[str] | None = None,\n        presence_penalty: float | None = None,\n        frequency_penalty: float | None = None,\n        seed: int | None = None,\n        user: str | None = None,\n        parallel_tool_calls: bool | None = None,\n        logprobs: bool | None = None,\n        top_logprobs: int | None = None,\n        logit_bias: dict[str, float] | None = None,\n        stream_options: dict[str, Any] | None = None,\n        max_completion_tokens: int | None = None,\n        reasoning_effort: Literal[\"minimal\", \"low\", \"medium\", \"high\", \"auto\"] | None = \"auto\",\n        **kwargs: Any,\n    ) -&gt; ChatCompletion | AsyncIterator[ChatCompletionChunk]:\n        \"\"\"Create a chat completion asynchronously.\n\n        Args:\n            model: Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).\n            messages: List of messages for the conversation\n            tools: List of tools for tool calling. Can be Python callables or OpenAI tool format dicts\n            tool_choice: Controls which tools the model can call\n            temperature: Controls randomness in the response (0.0 to 2.0)\n            top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n            max_tokens: Maximum number of tokens to generate\n            response_format: Format specification for the response\n            stream: Whether to stream the response\n            n: Number of completions to generate\n            stop: Stop sequences for generation\n            presence_penalty: Penalize new tokens based on presence in text\n            frequency_penalty: Penalize new tokens based on frequency in text\n            seed: Random seed for reproducible results\n            user: Unique identifier for the end user\n            parallel_tool_calls: Whether to allow parallel tool calls\n            logprobs: Include token-level log probabilities in the response\n            top_logprobs: Number of alternatives to return when logprobs are requested\n            logit_bias: Bias the likelihood of specified tokens during generation\n            stream_options: Additional options controlling streaming behavior\n            max_completion_tokens: Maximum number of tokens for the completion\n            reasoning_effort: Reasoning effort level for models that support it. \"auto\" will map to each provider's default.\n            **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n        Returns:\n            The completion response from the provider\n\n        \"\"\"\n        all_args = locals()\n        all_args.pop(\"self\")\n        all_args[\"model_id\"] = all_args.pop(\"model\")\n        kwargs = all_args.pop(\"kwargs\")\n\n        if tools:\n            all_args[\"tools\"] = prepare_tools(tools, built_in_tools=self.BUILT_IN_TOOLS)\n\n        for i, message in enumerate(messages):\n            if isinstance(message, ChatCompletionMessage):\n                # Dump the message but exclude the extra field that we extend from OpenAI Spec\n                messages[i] = message.model_dump(exclude_none=True, exclude={\"reasoning\"})\n        all_args[\"messages\"] = messages\n\n        return await self._acompletion(CompletionParams(**all_args), **kwargs)\n\n    async def _acompletion(\n        self, params: CompletionParams, **kwargs: Any\n    ) -&gt; ChatCompletion | AsyncIterator[ChatCompletionChunk]:\n        if not self.SUPPORTS_COMPLETION:\n            msg = \"Provider doesn't support completion.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _acompletion method\"\n        raise NotImplementedError(msg)\n\n    def responses(self, **kwargs: Any) -&gt; Response | Iterator[ResponseStreamEvent]:\n        \"\"\"Create a response synchronously.\n\n        See [AnyLLM.aresponses][any_llm.any_llm.AnyLLM.aresponses]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        response = run_async_in_sync(self.aresponses(**kwargs), allow_running_loop=allow_running_loop)\n        if isinstance(response, Response):\n            return response\n        return async_iter_to_sync_iter(response)\n\n    async def aresponses(\n        self,\n        model: str,\n        input_data: str | ResponseInputParam,\n        *,\n        tools: list[dict[str, Any] | Callable[..., Any]] | Any | None = None,\n        tool_choice: str | dict[str, Any] | None = None,\n        max_output_tokens: int | None = None,\n        temperature: float | None = None,\n        top_p: float | None = None,\n        stream: bool | None = None,\n        instructions: str | None = None,\n        max_tool_calls: int | None = None,\n        parallel_tool_calls: int | None = None,\n        reasoning: Any | None = None,\n        text: Any | None = None,\n        **kwargs: Any,\n    ) -&gt; Response | AsyncIterator[ResponseStreamEvent]:\n        \"\"\"Create a response using the OpenAI-style Responses API.\n\n        This follows the OpenAI Responses API shape and returns the aliased\n        `any_llm.types.responses.Response` type. If `stream=True`, an iterator of\n        `any_llm.types.responses.ResponseStreamEvent` items is returned.\n\n        Args:\n            model: Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).\n            input_data: The input payload accepted by provider's Responses API.\n                For OpenAI-compatible providers, this is typically a list mixing\n                text, images, and tool instructions, or a dict per OpenAI spec.\n            tools: Optional tools for tool calling (Python callables or OpenAI tool dicts)\n            tool_choice: Controls which tools the model can call\n            max_output_tokens: Maximum number of output tokens to generate\n            temperature: Controls randomness in the response (0.0 to 2.0)\n            top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n            stream: Whether to stream response events\n            instructions: A system (or developer) message inserted into the model's context.\n            max_tool_calls: The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.\n            parallel_tool_calls: Whether to allow the model to run tool calls in parallel.\n            reasoning: Configuration options for reasoning models.\n            text: Configuration options for a text response from the model. Can be plain text or structured JSON data.\n            **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n        Returns:\n            Either a `Response` object (non-streaming) or an iterator of\n            `ResponseStreamEvent` (streaming).\n\n        Raises:\n            NotImplementedError: If the selected provider does not support the Responses API.\n\n        \"\"\"\n        all_args = locals()\n        all_args.pop(\"self\")\n        all_args[\"input\"] = all_args.pop(\"input_data\")\n        kwargs = all_args.pop(\"kwargs\")\n\n        if tools:\n            all_args[\"tools\"] = prepare_tools(tools, built_in_tools=self.BUILT_IN_TOOLS)\n\n        return await self._aresponses(ResponsesParams(**all_args, **kwargs))\n\n    async def _aresponses(\n        self, params: ResponsesParams, **kwargs: Any\n    ) -&gt; Response | AsyncIterator[ResponseStreamEvent]:\n        if not self.SUPPORTS_RESPONSES:\n            msg = \"Provider doesn't support responses.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _aresponses method\"\n        raise NotImplementedError(msg)\n\n    def _embedding(self, model: str, inputs: str | list[str], **kwargs: Any) -&gt; CreateEmbeddingResponse:\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(self.aembedding(model, inputs, **kwargs), allow_running_loop=allow_running_loop)\n\n    async def aembedding(self, model: str, inputs: str | list[str], **kwargs: Any) -&gt; CreateEmbeddingResponse:\n        return await self._aembedding(model, inputs, **kwargs)\n\n    async def _aembedding(self, model: str, inputs: str | list[str], **kwargs: Any) -&gt; CreateEmbeddingResponse:\n        if not self.SUPPORTS_EMBEDDING:\n            msg = \"Provider doesn't support embedding.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _aembedding method\"\n        raise NotImplementedError(msg)\n\n    def list_models(self, **kwargs: Any) -&gt; Sequence[Model]:\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(self.alist_models(**kwargs), allow_running_loop=allow_running_loop)\n\n    async def alist_models(self, **kwargs: Any) -&gt; Sequence[Model]:\n        return await self._alist_models(**kwargs)\n\n    async def _alist_models(self, **kwargs: Any) -&gt; Sequence[Model]:\n        if not self.SUPPORTS_LIST_MODELS:\n            msg = \"Provider doesn't support listing models.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _alist_models method\"\n        raise NotImplementedError(msg)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    def create_batch(self, **kwargs: Any) -&gt; Batch:\n        \"\"\"Create a batch synchronously.\n\n        See [AnyLLM.acreate_batch][any_llm.any_llm.AnyLLM.acreate_batch]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(self.acreate_batch(**kwargs), allow_running_loop=allow_running_loop)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    async def acreate_batch(\n        self,\n        input_file_path: str,\n        endpoint: str,\n        completion_window: str = \"24h\",\n        metadata: dict[str, str] | None = None,\n        **kwargs: Any,\n    ) -&gt; Batch:\n        \"\"\"Create a batch job asynchronously.\n\n        Args:\n            input_file_path: Path to a local file containing batch requests in JSONL format.\n            endpoint: The endpoint to be used for all requests (e.g., '/v1/chat/completions')\n            completion_window: The time frame within which the batch should be processed (default: '24h')\n            metadata: Optional custom metadata for the batch\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            The created batch object\n\n        \"\"\"\n        return await self._acreate_batch(\n            input_file_path=input_file_path,\n            endpoint=endpoint,\n            completion_window=completion_window,\n            metadata=metadata,\n            **kwargs,\n        )\n\n    async def _acreate_batch(\n        self,\n        input_file_path: str,\n        endpoint: str,\n        completion_window: str = \"24h\",\n        metadata: dict[str, str] | None = None,\n        **kwargs: Any,\n    ) -&gt; Batch:\n        if not self.SUPPORTS_BATCH:\n            msg = \"Provider doesn't support batch completions.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _acreate_batch method\"\n        raise NotImplementedError(msg)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    def retrieve_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        \"\"\"Retrieve a batch synchronously.\n\n        See [AnyLLM.aretrieve_batch][any_llm.any_llm.AnyLLM.aretrieve_batch]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(self.aretrieve_batch(batch_id, **kwargs), allow_running_loop=allow_running_loop)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    async def aretrieve_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        \"\"\"Retrieve a batch job asynchronously.\n\n        Args:\n            batch_id: The ID of the batch to retrieve\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            The batch object\n\n        \"\"\"\n        return await self._aretrieve_batch(batch_id, **kwargs)\n\n    async def _aretrieve_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        if not self.SUPPORTS_BATCH:\n            msg = \"Provider doesn't support batch completions.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _aretrieve_batch method\"\n        raise NotImplementedError(msg)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    def cancel_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        \"\"\"Cancel a batch synchronously.\n\n        See [AnyLLM.acancel_batch][any_llm.any_llm.AnyLLM.acancel_batch]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(self.acancel_batch(batch_id, **kwargs), allow_running_loop=allow_running_loop)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    async def acancel_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        \"\"\"Cancel a batch job asynchronously.\n\n        Args:\n            batch_id: The ID of the batch to cancel\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            The cancelled batch object\n\n        \"\"\"\n        return await self._acancel_batch(batch_id, **kwargs)\n\n    async def _acancel_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        if not self.SUPPORTS_BATCH:\n            msg = \"Provider doesn't support batch completions.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _acancel_batch method\"\n        raise NotImplementedError(msg)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    def list_batches(\n        self,\n        after: str | None = None,\n        limit: int | None = None,\n        **kwargs: Any,\n    ) -&gt; Sequence[Batch]:\n        \"\"\"List batches synchronously.\n\n        See [AnyLLM.alist_batches][any_llm.any_llm.AnyLLM.alist_batches]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(\n            self.alist_batches(after=after, limit=limit, **kwargs), allow_running_loop=allow_running_loop\n        )\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    async def alist_batches(\n        self,\n        after: str | None = None,\n        limit: int | None = None,\n        **kwargs: Any,\n    ) -&gt; Sequence[Batch]:\n        \"\"\"List batch jobs asynchronously.\n\n        Args:\n            after: A cursor for pagination. Returns batches after this batch ID.\n            limit: Maximum number of batches to return (default: 20)\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            A list of batch objects\n\n        \"\"\"\n        return await self._alist_batches(after=after, limit=limit, **kwargs)\n\n    async def _alist_batches(\n        self,\n        after: str | None = None,\n        limit: int | None = None,\n        **kwargs: Any,\n    ) -&gt; Sequence[Batch]:\n        if not self.SUPPORTS_BATCH:\n            msg = \"Provider doesn't support batch completions.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _alist_batches method\"\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.API_BASE","title":"<code>API_BASE = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>This is used to set the API base for the provider. It is not required but may prove useful for providers that have overridable api bases.</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.BUILT_IN_TOOLS","title":"<code>BUILT_IN_TOOLS = None</code>  <code>class-attribute</code>","text":"<p>Some providers have built-in tools that can be used as-is without conversion. This should be a list of the allowed built-in tool instances. For example, in <code>gemini</code> provider, this could include <code>google.genai.types.Tool</code>.</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.ENV_API_KEY_NAME","title":"<code>ENV_API_KEY_NAME</code>  <code>instance-attribute</code>","text":"<p>Environment variable name for the API key</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.MISSING_PACKAGES_ERROR","title":"<code>MISSING_PACKAGES_ERROR = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Some providers use SDKs that are not installed by default. This flag is used to check if the packages are installed before instantiating the provider.</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.PROVIDER_DOCUMENTATION_URL","title":"<code>PROVIDER_DOCUMENTATION_URL</code>  <code>instance-attribute</code>","text":"<p>Link to the provider's documentation</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.PROVIDER_NAME","title":"<code>PROVIDER_NAME</code>  <code>instance-attribute</code>","text":"<p>Must match the name of the provider directory  (case sensitive)</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_BATCH","title":"<code>SUPPORTS_BATCH</code>  <code>instance-attribute</code>","text":"<p>OpenAI Batch Completion API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_COMPLETION","title":"<code>SUPPORTS_COMPLETION</code>  <code>instance-attribute</code>","text":"<p>OpenAI Completion API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_COMPLETION_IMAGE","title":"<code>SUPPORTS_COMPLETION_IMAGE</code>  <code>instance-attribute</code>","text":"<p>Image Support for Completion API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_COMPLETION_PDF","title":"<code>SUPPORTS_COMPLETION_PDF</code>  <code>instance-attribute</code>","text":"<p>PDF Support for Completion API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_COMPLETION_REASONING","title":"<code>SUPPORTS_COMPLETION_REASONING</code>  <code>instance-attribute</code>","text":"<p>Reasoning Content attached to Completion API Response</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_COMPLETION_STREAMING","title":"<code>SUPPORTS_COMPLETION_STREAMING</code>  <code>instance-attribute</code>","text":"<p>OpenAI Streaming Completion API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_EMBEDDING","title":"<code>SUPPORTS_EMBEDDING</code>  <code>instance-attribute</code>","text":"<p>OpenAI Embedding API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_LIST_MODELS","title":"<code>SUPPORTS_LIST_MODELS</code>  <code>instance-attribute</code>","text":"<p>OpenAI Models API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_RESPONSES","title":"<code>SUPPORTS_RESPONSES</code>  <code>instance-attribute</code>","text":"<p>OpenAI Responses API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.acancel_batch","title":"<code>acancel_batch(batch_id, **kwargs)</code>  <code>async</code>","text":"<p>Cancel a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>batch_id</code> <code>str</code> <p>The ID of the batch to cancel</p> required <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The cancelled batch object</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def acancel_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n    \"\"\"Cancel a batch job asynchronously.\n\n    Args:\n        batch_id: The ID of the batch to cancel\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The cancelled batch object\n\n    \"\"\"\n    return await self._acancel_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.acompletion","title":"<code>acompletion(model, messages, *, tools=None, tool_choice=None, temperature=None, top_p=None, max_tokens=None, response_format=None, stream=None, n=None, stop=None, presence_penalty=None, frequency_penalty=None, seed=None, user=None, parallel_tool_calls=None, logprobs=None, top_logprobs=None, logit_bias=None, stream_options=None, max_completion_tokens=None, reasoning_effort='auto', **kwargs)</code>  <code>async</code>","text":"<p>Create a chat completion asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).</p> required <code>messages</code> <code>list[dict[str, Any] | ChatCompletionMessage]</code> <p>List of messages for the conversation</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | Any | None</code> <p>List of tools for tool calling. Can be Python callables or OpenAI tool format dicts</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>response_format</code> <code>dict[str, Any] | type[BaseModel] | None</code> <p>Format specification for the response</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream the response</p> <code>None</code> <code>n</code> <code>int | None</code> <p>Number of completions to generate</p> <code>None</code> <code>stop</code> <code>str | list[str] | None</code> <p>Stop sequences for generation</p> <code>None</code> <code>presence_penalty</code> <code>float | None</code> <p>Penalize new tokens based on presence in text</p> <code>None</code> <code>frequency_penalty</code> <code>float | None</code> <p>Penalize new tokens based on frequency in text</p> <code>None</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducible results</p> <code>None</code> <code>user</code> <code>str | None</code> <p>Unique identifier for the end user</p> <code>None</code> <code>parallel_tool_calls</code> <code>bool | None</code> <p>Whether to allow parallel tool calls</p> <code>None</code> <code>logprobs</code> <code>bool | None</code> <p>Include token-level log probabilities in the response</p> <code>None</code> <code>top_logprobs</code> <code>int | None</code> <p>Number of alternatives to return when logprobs are requested</p> <code>None</code> <code>logit_bias</code> <code>dict[str, float] | None</code> <p>Bias the likelihood of specified tokens during generation</p> <code>None</code> <code>stream_options</code> <code>dict[str, Any] | None</code> <p>Additional options controlling streaming behavior</p> <code>None</code> <code>max_completion_tokens</code> <code>int | None</code> <p>Maximum number of tokens for the completion</p> <code>None</code> <code>reasoning_effort</code> <code>Literal['minimal', 'low', 'medium', 'high', 'auto'] | None</code> <p>Reasoning effort level for models that support it. \"auto\" will map to each provider's default.</p> <code>'auto'</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletion | AsyncIterator[ChatCompletionChunk]</code> <p>The completion response from the provider</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>async def acompletion(\n    self,\n    model: str,\n    messages: list[dict[str, Any] | ChatCompletionMessage],\n    *,\n    tools: list[dict[str, Any] | Callable[..., Any]] | Any | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    max_tokens: int | None = None,\n    response_format: dict[str, Any] | type[BaseModel] | None = None,\n    stream: bool | None = None,\n    n: int | None = None,\n    stop: str | list[str] | None = None,\n    presence_penalty: float | None = None,\n    frequency_penalty: float | None = None,\n    seed: int | None = None,\n    user: str | None = None,\n    parallel_tool_calls: bool | None = None,\n    logprobs: bool | None = None,\n    top_logprobs: int | None = None,\n    logit_bias: dict[str, float] | None = None,\n    stream_options: dict[str, Any] | None = None,\n    max_completion_tokens: int | None = None,\n    reasoning_effort: Literal[\"minimal\", \"low\", \"medium\", \"high\", \"auto\"] | None = \"auto\",\n    **kwargs: Any,\n) -&gt; ChatCompletion | AsyncIterator[ChatCompletionChunk]:\n    \"\"\"Create a chat completion asynchronously.\n\n    Args:\n        model: Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).\n        messages: List of messages for the conversation\n        tools: List of tools for tool calling. Can be Python callables or OpenAI tool format dicts\n        tool_choice: Controls which tools the model can call\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        max_tokens: Maximum number of tokens to generate\n        response_format: Format specification for the response\n        stream: Whether to stream the response\n        n: Number of completions to generate\n        stop: Stop sequences for generation\n        presence_penalty: Penalize new tokens based on presence in text\n        frequency_penalty: Penalize new tokens based on frequency in text\n        seed: Random seed for reproducible results\n        user: Unique identifier for the end user\n        parallel_tool_calls: Whether to allow parallel tool calls\n        logprobs: Include token-level log probabilities in the response\n        top_logprobs: Number of alternatives to return when logprobs are requested\n        logit_bias: Bias the likelihood of specified tokens during generation\n        stream_options: Additional options controlling streaming behavior\n        max_completion_tokens: Maximum number of tokens for the completion\n        reasoning_effort: Reasoning effort level for models that support it. \"auto\" will map to each provider's default.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The completion response from the provider\n\n    \"\"\"\n    all_args = locals()\n    all_args.pop(\"self\")\n    all_args[\"model_id\"] = all_args.pop(\"model\")\n    kwargs = all_args.pop(\"kwargs\")\n\n    if tools:\n        all_args[\"tools\"] = prepare_tools(tools, built_in_tools=self.BUILT_IN_TOOLS)\n\n    for i, message in enumerate(messages):\n        if isinstance(message, ChatCompletionMessage):\n            # Dump the message but exclude the extra field that we extend from OpenAI Spec\n            messages[i] = message.model_dump(exclude_none=True, exclude={\"reasoning\"})\n    all_args[\"messages\"] = messages\n\n    return await self._acompletion(CompletionParams(**all_args), **kwargs)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.acreate_batch","title":"<code>acreate_batch(input_file_path, endpoint, completion_window='24h', metadata=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>input_file_path</code> <code>str</code> <p>Path to a local file containing batch requests in JSONL format.</p> required <code>endpoint</code> <code>str</code> <p>The endpoint to be used for all requests (e.g., '/v1/chat/completions')</p> required <code>completion_window</code> <code>str</code> <p>The time frame within which the batch should be processed (default: '24h')</p> <code>'24h'</code> <code>metadata</code> <code>dict[str, str] | None</code> <p>Optional custom metadata for the batch</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The created batch object</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def acreate_batch(\n    self,\n    input_file_path: str,\n    endpoint: str,\n    completion_window: str = \"24h\",\n    metadata: dict[str, str] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Create a batch job asynchronously.\n\n    Args:\n        input_file_path: Path to a local file containing batch requests in JSONL format.\n        endpoint: The endpoint to be used for all requests (e.g., '/v1/chat/completions')\n        completion_window: The time frame within which the batch should be processed (default: '24h')\n        metadata: Optional custom metadata for the batch\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The created batch object\n\n    \"\"\"\n    return await self._acreate_batch(\n        input_file_path=input_file_path,\n        endpoint=endpoint,\n        completion_window=completion_window,\n        metadata=metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.alist_batches","title":"<code>alist_batches(after=None, limit=None, **kwargs)</code>  <code>async</code>","text":"<p>List batch jobs asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | None</code> <p>A cursor for pagination. Returns batches after this batch ID.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Maximum number of batches to return (default: 20)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sequence[Batch]</code> <p>A list of batch objects</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def alist_batches(\n    self,\n    after: str | None = None,\n    limit: int | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Batch]:\n    \"\"\"List batch jobs asynchronously.\n\n    Args:\n        after: A cursor for pagination. Returns batches after this batch ID.\n        limit: Maximum number of batches to return (default: 20)\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        A list of batch objects\n\n    \"\"\"\n    return await self._alist_batches(after=after, limit=limit, **kwargs)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.aresponses","title":"<code>aresponses(model, input_data, *, tools=None, tool_choice=None, max_output_tokens=None, temperature=None, top_p=None, stream=None, instructions=None, max_tool_calls=None, parallel_tool_calls=None, reasoning=None, text=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a response using the OpenAI-style Responses API.</p> <p>This follows the OpenAI Responses API shape and returns the aliased <code>any_llm.types.responses.Response</code> type. If <code>stream=True</code>, an iterator of <code>any_llm.types.responses.ResponseStreamEvent</code> items is returned.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).</p> required <code>input_data</code> <code>str | ResponseInputParam</code> <p>The input payload accepted by provider's Responses API. For OpenAI-compatible providers, this is typically a list mixing text, images, and tool instructions, or a dict per OpenAI spec.</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | Any | None</code> <p>Optional tools for tool calling (Python callables or OpenAI tool dicts)</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>max_output_tokens</code> <code>int | None</code> <p>Maximum number of output tokens to generate</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream response events</p> <code>None</code> <code>instructions</code> <code>str | None</code> <p>A system (or developer) message inserted into the model's context.</p> <code>None</code> <code>max_tool_calls</code> <code>int | None</code> <p>The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.</p> <code>None</code> <code>parallel_tool_calls</code> <code>int | None</code> <p>Whether to allow the model to run tool calls in parallel.</p> <code>None</code> <code>reasoning</code> <code>Any | None</code> <p>Configuration options for reasoning models.</p> <code>None</code> <code>text</code> <code>Any | None</code> <p>Configuration options for a text response from the model. Can be plain text or structured JSON data.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response | AsyncIterator[ResponseStreamEvent]</code> <p>Either a <code>Response</code> object (non-streaming) or an iterator of</p> <code>Response | AsyncIterator[ResponseStreamEvent]</code> <p><code>ResponseStreamEvent</code> (streaming).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the selected provider does not support the Responses API.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>async def aresponses(\n    self,\n    model: str,\n    input_data: str | ResponseInputParam,\n    *,\n    tools: list[dict[str, Any] | Callable[..., Any]] | Any | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    max_output_tokens: int | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    stream: bool | None = None,\n    instructions: str | None = None,\n    max_tool_calls: int | None = None,\n    parallel_tool_calls: int | None = None,\n    reasoning: Any | None = None,\n    text: Any | None = None,\n    **kwargs: Any,\n) -&gt; Response | AsyncIterator[ResponseStreamEvent]:\n    \"\"\"Create a response using the OpenAI-style Responses API.\n\n    This follows the OpenAI Responses API shape and returns the aliased\n    `any_llm.types.responses.Response` type. If `stream=True`, an iterator of\n    `any_llm.types.responses.ResponseStreamEvent` items is returned.\n\n    Args:\n        model: Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).\n        input_data: The input payload accepted by provider's Responses API.\n            For OpenAI-compatible providers, this is typically a list mixing\n            text, images, and tool instructions, or a dict per OpenAI spec.\n        tools: Optional tools for tool calling (Python callables or OpenAI tool dicts)\n        tool_choice: Controls which tools the model can call\n        max_output_tokens: Maximum number of output tokens to generate\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        stream: Whether to stream response events\n        instructions: A system (or developer) message inserted into the model's context.\n        max_tool_calls: The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.\n        parallel_tool_calls: Whether to allow the model to run tool calls in parallel.\n        reasoning: Configuration options for reasoning models.\n        text: Configuration options for a text response from the model. Can be plain text or structured JSON data.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        Either a `Response` object (non-streaming) or an iterator of\n        `ResponseStreamEvent` (streaming).\n\n    Raises:\n        NotImplementedError: If the selected provider does not support the Responses API.\n\n    \"\"\"\n    all_args = locals()\n    all_args.pop(\"self\")\n    all_args[\"input\"] = all_args.pop(\"input_data\")\n    kwargs = all_args.pop(\"kwargs\")\n\n    if tools:\n        all_args[\"tools\"] = prepare_tools(tools, built_in_tools=self.BUILT_IN_TOOLS)\n\n    return await self._aresponses(ResponsesParams(**all_args, **kwargs))\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.aretrieve_batch","title":"<code>aretrieve_batch(batch_id, **kwargs)</code>  <code>async</code>","text":"<p>Retrieve a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>batch_id</code> <code>str</code> <p>The ID of the batch to retrieve</p> required <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The batch object</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def aretrieve_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n    \"\"\"Retrieve a batch job asynchronously.\n\n    Args:\n        batch_id: The ID of the batch to retrieve\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The batch object\n\n    \"\"\"\n    return await self._aretrieve_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.cancel_batch","title":"<code>cancel_batch(batch_id, **kwargs)</code>","text":"<p>Cancel a batch synchronously.</p> <p>See AnyLLM.acancel_batch</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef cancel_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n    \"\"\"Cancel a batch synchronously.\n\n    See [AnyLLM.acancel_batch][any_llm.any_llm.AnyLLM.acancel_batch]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    return run_async_in_sync(self.acancel_batch(batch_id, **kwargs), allow_running_loop=allow_running_loop)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.completion","title":"<code>completion(**kwargs)</code>","text":"<p>Create a chat completion synchronously.</p> <p>See AnyLLM.acompletion</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>def completion(\n    self,\n    **kwargs: Any,\n) -&gt; ChatCompletion | Iterator[ChatCompletionChunk]:\n    \"\"\"Create a chat completion synchronously.\n\n    See [AnyLLM.acompletion][any_llm.any_llm.AnyLLM.acompletion]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    response = run_async_in_sync(self.acompletion(**kwargs), allow_running_loop=allow_running_loop)\n    if isinstance(response, ChatCompletion):\n        return response\n\n    return async_iter_to_sync_iter(response)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.create","title":"<code>create(provider, api_key=None, api_base=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a provider instance using the given provider name and config.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>The provider name (e.g., 'openai', 'anthropic')</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>AnyLLM</code> <p>Provider instance for the specified provider</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef create(\n    cls, provider: str | LLMProvider, api_key: str | None = None, api_base: str | None = None, **kwargs: Any\n) -&gt; AnyLLM:\n    \"\"\"Create a provider instance using the given provider name and config.\n\n    Args:\n        provider: The provider name (e.g., 'openai', 'anthropic')\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        Provider instance for the specified provider\n\n    \"\"\"\n    return cls._create_provider(provider, api_key=api_key, api_base=api_base, **kwargs)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.create_batch","title":"<code>create_batch(**kwargs)</code>","text":"<p>Create a batch synchronously.</p> <p>See AnyLLM.acreate_batch</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef create_batch(self, **kwargs: Any) -&gt; Batch:\n    \"\"\"Create a batch synchronously.\n\n    See [AnyLLM.acreate_batch][any_llm.any_llm.AnyLLM.acreate_batch]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    return run_async_in_sync(self.acreate_batch(**kwargs), allow_running_loop=allow_running_loop)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.get_all_provider_metadata","title":"<code>get_all_provider_metadata()</code>  <code>classmethod</code>","text":"<p>Get metadata for all supported providers.</p> <p>Returns:</p> Type Description <code>list[ProviderMetadata]</code> <p>List of dictionaries containing provider metadata</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef get_all_provider_metadata(cls) -&gt; list[ProviderMetadata]:\n    \"\"\"Get metadata for all supported providers.\n\n    Returns:\n        List of dictionaries containing provider metadata\n\n    \"\"\"\n    providers: list[ProviderMetadata] = []\n    for provider_key in cls.get_supported_providers():\n        provider_class = cls.get_provider_class(provider_key)\n        metadata = provider_class.get_provider_metadata()\n        providers.append(metadata)\n\n    # Sort providers by name\n    providers.sort(key=lambda x: x.name)\n    return providers\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.get_provider_class","title":"<code>get_provider_class(provider_key)</code>  <code>classmethod</code>","text":"<p>Get the provider class without instantiating it.</p> <p>Parameters:</p> Name Type Description Default <code>provider_key</code> <code>str | LLMProvider</code> <p>The provider key (e.g., 'anthropic', 'openai')</p> required <p>Returns:</p> Type Description <code>type[AnyLLM]</code> <p>The provider class</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef get_provider_class(cls, provider_key: str | LLMProvider) -&gt; type[AnyLLM]:\n    \"\"\"Get the provider class without instantiating it.\n\n    Args:\n        provider_key: The provider key (e.g., 'anthropic', 'openai')\n\n    Returns:\n        The provider class\n\n    \"\"\"\n    provider_key = LLMProvider.from_string(provider_key).value\n\n    provider_class_name = f\"{provider_key.capitalize()}Provider\"\n    provider_module_name = f\"{provider_key}\"\n\n    module_path = f\"any_llm.providers.{provider_module_name}\"\n\n    try:\n        module = importlib.import_module(module_path)\n    except ImportError as e:\n        msg = f\"Could not import module {module_path}: {e!s}. Please ensure the provider is supported by doing AnyLLM.get_supported_providers()\"\n        raise ImportError(msg) from e\n\n    provider_class: type[AnyLLM] = getattr(module, provider_class_name)\n    return provider_class\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.get_provider_enum","title":"<code>get_provider_enum(provider_key)</code>  <code>classmethod</code>","text":"<p>Convert a string provider key to a ProviderName enum.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef get_provider_enum(cls, provider_key: str) -&gt; LLMProvider:\n    \"\"\"Convert a string provider key to a ProviderName enum.\"\"\"\n    try:\n        return LLMProvider(provider_key)\n    except ValueError as e:\n        supported = [provider.value for provider in LLMProvider]\n        raise UnsupportedProviderError(provider_key, supported) from e\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.get_provider_metadata","title":"<code>get_provider_metadata()</code>  <code>classmethod</code>","text":"<p>Get provider metadata without requiring instantiation.</p> <p>Returns:</p> Type Description <code>ProviderMetadata</code> <p>Dictionary containing provider metadata including name, environment variable,</p> <code>ProviderMetadata</code> <p>documentation URL, and class name.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef get_provider_metadata(cls) -&gt; ProviderMetadata:\n    \"\"\"Get provider metadata without requiring instantiation.\n\n    Returns:\n        Dictionary containing provider metadata including name, environment variable,\n        documentation URL, and class name.\n\n    \"\"\"\n    return ProviderMetadata(\n        name=cls.PROVIDER_NAME,\n        env_key=cls.ENV_API_KEY_NAME,\n        doc_url=cls.PROVIDER_DOCUMENTATION_URL,\n        streaming=cls.SUPPORTS_COMPLETION_STREAMING,\n        reasoning=cls.SUPPORTS_COMPLETION_REASONING,\n        completion=cls.SUPPORTS_COMPLETION,\n        image=cls.SUPPORTS_COMPLETION_IMAGE,\n        pdf=cls.SUPPORTS_COMPLETION_PDF,\n        embedding=cls.SUPPORTS_EMBEDDING,\n        responses=cls.SUPPORTS_RESPONSES,\n        list_models=cls.SUPPORTS_LIST_MODELS,\n        batch_completion=cls.SUPPORTS_BATCH,\n        class_name=cls.__name__,\n    )\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.get_supported_providers","title":"<code>get_supported_providers()</code>  <code>classmethod</code>","text":"<p>Get a list of supported provider keys.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef get_supported_providers(cls) -&gt; list[str]:\n    \"\"\"Get a list of supported provider keys.\"\"\"\n    return [provider.value for provider in LLMProvider]\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.list_batches","title":"<code>list_batches(after=None, limit=None, **kwargs)</code>","text":"<p>List batches synchronously.</p> <p>See AnyLLM.alist_batches</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef list_batches(\n    self,\n    after: str | None = None,\n    limit: int | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Batch]:\n    \"\"\"List batches synchronously.\n\n    See [AnyLLM.alist_batches][any_llm.any_llm.AnyLLM.alist_batches]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    return run_async_in_sync(\n        self.alist_batches(after=after, limit=limit, **kwargs), allow_running_loop=allow_running_loop\n    )\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.responses","title":"<code>responses(**kwargs)</code>","text":"<p>Create a response synchronously.</p> <p>See AnyLLM.aresponses</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>def responses(self, **kwargs: Any) -&gt; Response | Iterator[ResponseStreamEvent]:\n    \"\"\"Create a response synchronously.\n\n    See [AnyLLM.aresponses][any_llm.any_llm.AnyLLM.aresponses]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    response = run_async_in_sync(self.aresponses(**kwargs), allow_running_loop=allow_running_loop)\n    if isinstance(response, Response):\n        return response\n    return async_iter_to_sync_iter(response)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.retrieve_batch","title":"<code>retrieve_batch(batch_id, **kwargs)</code>","text":"<p>Retrieve a batch synchronously.</p> <p>See AnyLLM.aretrieve_batch</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef retrieve_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n    \"\"\"Retrieve a batch synchronously.\n\n    See [AnyLLM.aretrieve_batch][any_llm.any_llm.AnyLLM.aretrieve_batch]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    return run_async_in_sync(self.aretrieve_batch(batch_id, **kwargs), allow_running_loop=allow_running_loop)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.split_model_provider","title":"<code>split_model_provider(model)</code>  <code>classmethod</code>","text":"<p>Extract the provider key from the model identifier.</p> <p>Supports both new format 'provider:model' (e.g., 'mistral:mistral-small') and legacy format 'provider/model' (e.g., 'mistral/mistral-small').</p> <p>The legacy format will be deprecated in version 1.0.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef split_model_provider(cls, model: str) -&gt; tuple[LLMProvider, str]:\n    \"\"\"Extract the provider key from the model identifier.\n\n    Supports both new format 'provider:model' (e.g., 'mistral:mistral-small')\n    and legacy format 'provider/model' (e.g., 'mistral/mistral-small').\n\n    The legacy format will be deprecated in version 1.0.\n    \"\"\"\n    colon_index = model.find(\":\")\n    slash_index = model.find(\"/\")\n\n    # Determine which delimiter comes first\n    if colon_index != -1 and (slash_index == -1 or colon_index &lt; slash_index):\n        # The colon came first, so it's using the new syntax.\n        provider, model_name = model.split(\":\", 1)\n    elif slash_index != -1:\n        # Slash comes first, so it's the legacy syntax\n        warnings.warn(\n            f\"Model format 'provider/model' is deprecated and will be removed in version 1.0. \"\n            f\"Please use 'provider:model' format instead. Got: '{model}'\",\n            DeprecationWarning,\n            stacklevel=3,\n        )\n        provider, model_name = model.split(\"/\", 1)\n    else:\n        msg = f\"Invalid model format. Expected 'provider:model' or 'provider/model', got '{model}'\"\n        raise ValueError(msg)\n\n    if not provider or not model_name:\n        msg = f\"Invalid model format. Expected 'provider:model' or 'provider/model', got '{model}'\"\n        raise ValueError(msg)\n    return cls.get_provider_enum(provider), model_name\n</code></pre>"},{"location":"api/batch/","title":"Batch","text":"<p>Experimental API</p> <p>The Batch API is experimental and subject to breaking changes in future versions. Use with caution in production environments.</p> <p>The Batch API allows you to process multiple requests asynchronously at a lower cost.</p>"},{"location":"api/batch/#file-path-interface","title":"File Path Interface","text":"<p>The <code>any-llm</code> batch API requires you to pass a path to a local JSONL file containing your batch requests. The provider implementation automatically handles uploading and file management as needed.</p> <p>Different providers handle batch processing differently:</p> <ul> <li>OpenAI: Requires uploading a file first, then creating a batch with the file ID</li> <li>Anthropic (future): Expects file content passed directly in the request</li> <li>Other providers: May have their own unique requirements</li> </ul> <p>By accepting a local file path, <code>any-llm</code> abstracts these provider differences and handles the implementation details automatically.</p>"},{"location":"api/batch/#any_llm.api.create_batch","title":"<code>any_llm.api.create_batch(provider, input_file_path, endpoint, *, completion_window='24h', metadata=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>Create a batch job.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>input_file_path</code> <code>str</code> <p>Path to a local file containing batch requests in JSONL format.</p> required <code>endpoint</code> <code>str</code> <p>The endpoint to be used for all requests (e.g., '/v1/chat/completions')</p> required <code>completion_window</code> <code>str</code> <p>The time frame within which the batch should be processed (default: '24h')</p> <code>'24h'</code> <code>metadata</code> <code>dict[str, str] | None</code> <p>Optional custom metadata for the batch</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The created batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef create_batch(\n    provider: str | LLMProvider,\n    input_file_path: str,\n    endpoint: str,\n    *,\n    completion_window: str = \"24h\",\n    metadata: dict[str, str] | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Create a batch job.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        input_file_path: Path to a local file containing batch requests in JSONL format.\n        endpoint: The endpoint to be used for all requests (e.g., '/v1/chat/completions')\n        completion_window: The time frame within which the batch should be processed (default: '24h')\n        metadata: Optional custom metadata for the batch\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The created batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return llm.create_batch(\n        input_file_path=input_file_path,\n        endpoint=endpoint,\n        completion_window=completion_window,\n        metadata=metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/batch/#any_llm.api.acreate_batch","title":"<code>any_llm.api.acreate_batch(provider, input_file_path, endpoint, *, completion_window='24h', metadata=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>input_file_path</code> <code>str</code> <p>Path to a local file containing batch requests in JSONL format.</p> required <code>endpoint</code> <code>str</code> <p>The endpoint to be used for all requests (e.g., '/v1/chat/completions')</p> required <code>completion_window</code> <code>str</code> <p>The time frame within which the batch should be processed (default: '24h')</p> <code>'24h'</code> <code>metadata</code> <code>dict[str, str] | None</code> <p>Optional custom metadata for the batch</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The created batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def acreate_batch(\n    provider: str | LLMProvider,\n    input_file_path: str,\n    endpoint: str,\n    *,\n    completion_window: str = \"24h\",\n    metadata: dict[str, str] | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Create a batch job asynchronously.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        input_file_path: Path to a local file containing batch requests in JSONL format.\n        endpoint: The endpoint to be used for all requests (e.g., '/v1/chat/completions')\n        completion_window: The time frame within which the batch should be processed (default: '24h')\n        metadata: Optional custom metadata for the batch\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The created batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm.acreate_batch(\n        input_file_path=input_file_path,\n        endpoint=endpoint,\n        completion_window=completion_window,\n        metadata=metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/batch/#any_llm.api.retrieve_batch","title":"<code>any_llm.api.retrieve_batch(provider, batch_id, *, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>Retrieve a batch job.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>batch_id</code> <code>str</code> <p>The ID of the batch to retrieve</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef retrieve_batch(\n    provider: str | LLMProvider,\n    batch_id: str,\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Retrieve a batch job.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        batch_id: The ID of the batch to retrieve\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return llm.retrieve_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/batch/#any_llm.api.aretrieve_batch","title":"<code>any_llm.api.aretrieve_batch(provider, batch_id, *, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Retrieve a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>batch_id</code> <code>str</code> <p>The ID of the batch to retrieve</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def aretrieve_batch(\n    provider: str | LLMProvider,\n    batch_id: str,\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Retrieve a batch job asynchronously.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        batch_id: The ID of the batch to retrieve\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm.aretrieve_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/batch/#any_llm.api.cancel_batch","title":"<code>any_llm.api.cancel_batch(provider, batch_id, *, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>Cancel a batch job.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>batch_id</code> <code>str</code> <p>The ID of the batch to cancel</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The cancelled batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef cancel_batch(\n    provider: str | LLMProvider,\n    batch_id: str,\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Cancel a batch job.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        batch_id: The ID of the batch to cancel\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The cancelled batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return llm.cancel_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/batch/#any_llm.api.acancel_batch","title":"<code>any_llm.api.acancel_batch(provider, batch_id, *, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Cancel a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>batch_id</code> <code>str</code> <p>The ID of the batch to cancel</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The cancelled batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def acancel_batch(\n    provider: str | LLMProvider,\n    batch_id: str,\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Cancel a batch job asynchronously.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        batch_id: The ID of the batch to cancel\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The cancelled batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm.acancel_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/batch/#any_llm.api.list_batches","title":"<code>any_llm.api.list_batches(provider, *, after=None, limit=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>List batch jobs.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>after</code> <code>str | None</code> <p>A cursor for pagination. Returns batches after this batch ID.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Maximum number of batches to return (default: 20)</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sequence[Batch]</code> <p>A list of batch objects</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef list_batches(\n    provider: str | LLMProvider,\n    *,\n    after: str | None = None,\n    limit: int | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Batch]:\n    \"\"\"List batch jobs.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        after: A cursor for pagination. Returns batches after this batch ID.\n        limit: Maximum number of batches to return (default: 20)\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        A list of batch objects\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return llm.list_batches(after=after, limit=limit, **kwargs)\n</code></pre>"},{"location":"api/batch/#any_llm.api.alist_batches","title":"<code>any_llm.api.alist_batches(provider, *, after=None, limit=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>List batch jobs asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>after</code> <code>str | None</code> <p>A cursor for pagination. Returns batches after this batch ID.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Maximum number of batches to return (default: 20)</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sequence[Batch]</code> <p>A list of batch objects</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def alist_batches(\n    provider: str | LLMProvider,\n    *,\n    after: str | None = None,\n    limit: int | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Batch]:\n    \"\"\"List batch jobs asynchronously.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        after: A cursor for pagination. Returns batches after this batch ID.\n        limit: Maximum number of batches to return (default: 20)\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        A list of batch objects\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm.alist_batches(after=after, limit=limit, **kwargs)\n</code></pre>"},{"location":"api/completion/","title":"Completion","text":""},{"location":"api/completion/#completion","title":"Completion","text":""},{"location":"api/completion/#any_llm.api.completion","title":"<code>any_llm.api.completion(model, messages, *, provider=None, tools=None, tool_choice=None, temperature=None, top_p=None, max_tokens=None, response_format=None, stream=None, n=None, stop=None, presence_penalty=None, frequency_penalty=None, seed=None, api_key=None, api_base=None, user=None, parallel_tool_calls=None, logprobs=None, top_logprobs=None, logit_bias=None, stream_options=None, max_completion_tokens=None, reasoning_effort='auto', client_args=None, **kwargs)</code>","text":"<p>Create a chat completion.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier. Recommended: Use with separate <code>provider</code> parameter (e.g., model='gpt-4', provider='openai'). Alternative: Combined format 'provider:model' (e.g., 'openai:gpt-4'). Legacy format 'provider/model' is also supported but deprecated.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Recommended: Provider name to use for the request (e.g., 'openai', 'mistral'). When provided, the model parameter should contain only the model name.</p> <code>None</code> <code>messages</code> <code>list[dict[str, Any] | ChatCompletionMessage]</code> <p>List of messages for the conversation</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>List of tools for tool calling. Can be Python callables or OpenAI tool format dicts</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>response_format</code> <code>dict[str, Any] | type[BaseModel] | None</code> <p>Format specification for the response</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream the response</p> <code>None</code> <code>n</code> <code>int | None</code> <p>Number of completions to generate</p> <code>None</code> <code>stop</code> <code>str | list[str] | None</code> <p>Stop sequences for generation</p> <code>None</code> <code>presence_penalty</code> <code>float | None</code> <p>Penalize new tokens based on presence in text</p> <code>None</code> <code>frequency_penalty</code> <code>float | None</code> <p>Penalize new tokens based on frequency in text</p> <code>None</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducible results</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>user</code> <code>str | None</code> <p>Unique identifier for the end user</p> <code>None</code> <code>parallel_tool_calls</code> <code>bool | None</code> <p>Whether to allow parallel tool calls</p> <code>None</code> <code>logprobs</code> <code>bool | None</code> <p>Include token-level log probabilities in the response</p> <code>None</code> <code>top_logprobs</code> <code>int | None</code> <p>Number of alternatives to return when logprobs are requested</p> <code>None</code> <code>logit_bias</code> <code>dict[str, float] | None</code> <p>Bias the likelihood of specified tokens during generation</p> <code>None</code> <code>stream_options</code> <code>dict[str, Any] | None</code> <p>Additional options controlling streaming behavior</p> <code>None</code> <code>max_completion_tokens</code> <code>int | None</code> <p>Maximum number of tokens for the completion</p> <code>None</code> <code>reasoning_effort</code> <code>Literal['minimal', 'low', 'medium', 'high', 'auto'] | None</code> <p>Reasoning effort level for models that support it. \"auto\" will map to each provider's default.</p> <code>'auto'</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletion | Iterator[ChatCompletionChunk]</code> <p>The completion response from the provider</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def completion(\n    model: str,\n    messages: list[dict[str, Any] | ChatCompletionMessage],\n    *,\n    provider: str | LLMProvider | None = None,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    max_tokens: int | None = None,\n    response_format: dict[str, Any] | type[BaseModel] | None = None,\n    stream: bool | None = None,\n    n: int | None = None,\n    stop: str | list[str] | None = None,\n    presence_penalty: float | None = None,\n    frequency_penalty: float | None = None,\n    seed: int | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    user: str | None = None,\n    parallel_tool_calls: bool | None = None,\n    logprobs: bool | None = None,\n    top_logprobs: int | None = None,\n    logit_bias: dict[str, float] | None = None,\n    stream_options: dict[str, Any] | None = None,\n    max_completion_tokens: int | None = None,\n    reasoning_effort: Literal[\"minimal\", \"low\", \"medium\", \"high\", \"auto\"] | None = \"auto\",\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; ChatCompletion | Iterator[ChatCompletionChunk]:\n    \"\"\"Create a chat completion.\n\n    Args:\n        model: Model identifier. **Recommended**: Use with separate `provider` parameter (e.g., model='gpt-4', provider='openai').\n            **Alternative**: Combined format 'provider:model' (e.g., 'openai:gpt-4').\n            Legacy format 'provider/model' is also supported but deprecated.\n        provider: **Recommended**: Provider name to use for the request (e.g., 'openai', 'mistral').\n            When provided, the model parameter should contain only the model name.\n        messages: List of messages for the conversation\n        tools: List of tools for tool calling. Can be Python callables or OpenAI tool format dicts\n        tool_choice: Controls which tools the model can call\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        max_tokens: Maximum number of tokens to generate\n        response_format: Format specification for the response\n        stream: Whether to stream the response\n        n: Number of completions to generate\n        stop: Stop sequences for generation\n        presence_penalty: Penalize new tokens based on presence in text\n        frequency_penalty: Penalize new tokens based on frequency in text\n        seed: Random seed for reproducible results\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        user: Unique identifier for the end user\n        parallel_tool_calls: Whether to allow parallel tool calls\n        logprobs: Include token-level log probabilities in the response\n        top_logprobs: Number of alternatives to return when logprobs are requested\n        logit_bias: Bias the likelihood of specified tokens during generation\n        stream_options: Additional options controlling streaming behavior\n        max_completion_tokens: Maximum number of tokens for the completion\n        reasoning_effort: Reasoning effort level for models that support it. \"auto\" will map to each provider's default.\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The completion response from the provider\n\n    \"\"\"\n    all_args = locals()\n    all_args.pop(\"provider\")\n    kwargs = all_args.pop(\"kwargs\")\n\n    model = all_args.pop(\"model\")\n    if provider is None:\n        provider_key, model_id = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_id = model\n    all_args[\"model\"] = model_id\n\n    llm = AnyLLM.create(\n        provider_key,\n        api_key=all_args.pop(\"api_key\"),\n        api_base=all_args.pop(\"api_base\"),\n        **all_args.pop(\"client_args\") or {},\n    )\n    return llm.completion(**all_args, **kwargs)\n</code></pre>"},{"location":"api/completion/#any_llm.api.acompletion","title":"<code>any_llm.api.acompletion(model, messages, *, provider=None, tools=None, tool_choice=None, temperature=None, top_p=None, max_tokens=None, response_format=None, stream=None, n=None, stop=None, presence_penalty=None, frequency_penalty=None, seed=None, api_key=None, api_base=None, user=None, parallel_tool_calls=None, logprobs=None, top_logprobs=None, logit_bias=None, stream_options=None, max_completion_tokens=None, reasoning_effort='auto', client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a chat completion asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier. Recommended: Use with separate <code>provider</code> parameter (e.g., model='gpt-4', provider='openai'). Alternative: Combined format 'provider:model' (e.g., 'openai:gpt-4'). Legacy format 'provider/model' is also supported but deprecated.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Recommended: Provider name to use for the request (e.g., 'openai', 'mistral'). When provided, the model parameter should contain only the model name.</p> <code>None</code> <code>messages</code> <code>list[dict[str, Any] | ChatCompletionMessage]</code> <p>List of messages for the conversation</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>List of tools for tool calling. Can be Python callables or OpenAI tool format dicts</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>response_format</code> <code>dict[str, Any] | type[BaseModel] | None</code> <p>Format specification for the response</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream the response</p> <code>None</code> <code>n</code> <code>int | None</code> <p>Number of completions to generate</p> <code>None</code> <code>stop</code> <code>str | list[str] | None</code> <p>Stop sequences for generation</p> <code>None</code> <code>presence_penalty</code> <code>float | None</code> <p>Penalize new tokens based on presence in text</p> <code>None</code> <code>frequency_penalty</code> <code>float | None</code> <p>Penalize new tokens based on frequency in text</p> <code>None</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducible results</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>user</code> <code>str | None</code> <p>Unique identifier for the end user</p> <code>None</code> <code>parallel_tool_calls</code> <code>bool | None</code> <p>Whether to allow parallel tool calls</p> <code>None</code> <code>logprobs</code> <code>bool | None</code> <p>Include token-level log probabilities in the response</p> <code>None</code> <code>top_logprobs</code> <code>int | None</code> <p>Number of alternatives to return when logprobs are requested</p> <code>None</code> <code>logit_bias</code> <code>dict[str, float] | None</code> <p>Bias the likelihood of specified tokens during generation</p> <code>None</code> <code>stream_options</code> <code>dict[str, Any] | None</code> <p>Additional options controlling streaming behavior</p> <code>None</code> <code>max_completion_tokens</code> <code>int | None</code> <p>Maximum number of tokens for the completion</p> <code>None</code> <code>reasoning_effort</code> <code>Literal['minimal', 'low', 'medium', 'high', 'auto'] | None</code> <p>Reasoning effort level for models that support it. \"auto\" will map to each provider's default.</p> <code>'auto'</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletion | AsyncIterator[ChatCompletionChunk]</code> <p>The completion response from the provider</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def acompletion(\n    model: str,\n    messages: list[dict[str, Any] | ChatCompletionMessage],\n    *,\n    provider: str | LLMProvider | None = None,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    max_tokens: int | None = None,\n    response_format: dict[str, Any] | type[BaseModel] | None = None,\n    stream: bool | None = None,\n    n: int | None = None,\n    stop: str | list[str] | None = None,\n    presence_penalty: float | None = None,\n    frequency_penalty: float | None = None,\n    seed: int | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    user: str | None = None,\n    parallel_tool_calls: bool | None = None,\n    logprobs: bool | None = None,\n    top_logprobs: int | None = None,\n    logit_bias: dict[str, float] | None = None,\n    stream_options: dict[str, Any] | None = None,\n    max_completion_tokens: int | None = None,\n    reasoning_effort: Literal[\"minimal\", \"low\", \"medium\", \"high\", \"auto\"] | None = \"auto\",\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; ChatCompletion | AsyncIterator[ChatCompletionChunk]:\n    \"\"\"Create a chat completion asynchronously.\n\n    Args:\n        model: Model identifier. **Recommended**: Use with separate `provider` parameter (e.g., model='gpt-4', provider='openai').\n            **Alternative**: Combined format 'provider:model' (e.g., 'openai:gpt-4').\n            Legacy format 'provider/model' is also supported but deprecated.\n        provider: **Recommended**: Provider name to use for the request (e.g., 'openai', 'mistral').\n            When provided, the model parameter should contain only the model name.\n        messages: List of messages for the conversation\n        tools: List of tools for tool calling. Can be Python callables or OpenAI tool format dicts\n        tool_choice: Controls which tools the model can call\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        max_tokens: Maximum number of tokens to generate\n        response_format: Format specification for the response\n        stream: Whether to stream the response\n        n: Number of completions to generate\n        stop: Stop sequences for generation\n        presence_penalty: Penalize new tokens based on presence in text\n        frequency_penalty: Penalize new tokens based on frequency in text\n        seed: Random seed for reproducible results\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        user: Unique identifier for the end user\n        parallel_tool_calls: Whether to allow parallel tool calls\n        logprobs: Include token-level log probabilities in the response\n        top_logprobs: Number of alternatives to return when logprobs are requested\n        logit_bias: Bias the likelihood of specified tokens during generation\n        stream_options: Additional options controlling streaming behavior\n        max_completion_tokens: Maximum number of tokens for the completion\n        reasoning_effort: Reasoning effort level for models that support it. \"auto\" will map to each provider's default.\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The completion response from the provider\n\n    \"\"\"\n    all_args = locals()\n    all_args.pop(\"provider\")\n    kwargs = all_args.pop(\"kwargs\")\n\n    model = all_args.pop(\"model\")\n    if provider is None:\n        provider_key, model_id = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_id = model\n    all_args[\"model\"] = model_id\n\n    llm = AnyLLM.create(\n        provider_key,\n        api_key=all_args.pop(\"api_key\"),\n        api_base=all_args.pop(\"api_base\"),\n        **all_args.pop(\"client_args\") or {},\n    )\n    return await llm.acompletion(**all_args, **kwargs)\n</code></pre>"},{"location":"api/embedding/","title":"Embedding","text":""},{"location":"api/embedding/#embedding","title":"Embedding","text":""},{"location":"api/embedding/#any_llm.api.embedding","title":"<code>any_llm.api.embedding(model, inputs, *, provider=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>Create an embedding.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier. Recommended: Use with separate <code>provider</code> parameter (e.g., model='gpt-4', provider='openai'). Alternative: Combined format 'provider:model' (e.g., 'openai:gpt-4'). Legacy format 'provider/model' is also supported but deprecated.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Recommended: Provider name to use for the request (e.g., 'openai', 'mistral'). When provided, the model parameter should contain only the model name.</p> <code>None</code> <code>inputs</code> <code>str | list[str]</code> <p>The input text to embed</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CreateEmbeddingResponse</code> <p>The embedding of the input text</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def embedding(\n    model: str,\n    inputs: str | list[str],\n    *,\n    provider: str | LLMProvider | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; CreateEmbeddingResponse:\n    \"\"\"Create an embedding.\n\n    Args:\n        model: Model identifier. **Recommended**: Use with separate `provider` parameter (e.g., model='gpt-4', provider='openai').\n            **Alternative**: Combined format 'provider:model' (e.g., 'openai:gpt-4').\n            Legacy format 'provider/model' is also supported but deprecated.\n        provider: **Recommended**: Provider name to use for the request (e.g., 'openai', 'mistral').\n            When provided, the model parameter should contain only the model name.\n        inputs: The input text to embed\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The embedding of the input text\n\n    \"\"\"\n    if provider is None:\n        provider_key, model_name = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_name = model\n\n    llm = AnyLLM.create(provider_key, api_key=api_key, api_base=api_base, **client_args or {})\n    return llm._embedding(model_name, inputs, **kwargs)\n</code></pre>"},{"location":"api/embedding/#any_llm.api.aembedding","title":"<code>any_llm.api.aembedding(model, inputs, *, provider=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Create an embedding asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'openai/text-embedding-3-small'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.</p> <code>None</code> <code>inputs</code> <code>str | list[str]</code> <p>The input text to embed</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CreateEmbeddingResponse</code> <p>The embedding of the input text</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def aembedding(\n    model: str,\n    inputs: str | list[str],\n    *,\n    provider: str | LLMProvider | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; CreateEmbeddingResponse:\n    \"\"\"Create an embedding asynchronously.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'openai/text-embedding-3-small'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.\n        provider: Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.\n        inputs: The input text to embed\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The embedding of the input text\n\n    \"\"\"\n    if provider is None:\n        provider_key, model_name = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_name = model\n\n    llm = AnyLLM.create(provider_key, api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm._aembedding(model_name, inputs, **kwargs)\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions","text":""},{"location":"api/exceptions/#exceptions","title":"Exceptions","text":""},{"location":"api/exceptions/#any_llm.exceptions","title":"<code>any_llm.exceptions</code>","text":"<p>Custom exceptions for any-llm package.</p>"},{"location":"api/exceptions/#any_llm.exceptions.MissingApiKeyError","title":"<code>MissingApiKeyError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an API key is missing or not provided.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class MissingApiKeyError(Exception):\n    \"\"\"Exception raised when an API key is missing or not provided.\"\"\"\n\n    def __init__(self, provider_name: str, env_var_name: str) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            provider_name: Name of the provider (e.g., \"OpenAI\", \"Google\", \"Mistral\")\n            env_var_name: Name of the environment variable that should contain the API key\n\n        \"\"\"\n        self.provider_name = provider_name\n        self.env_var_name = env_var_name\n\n        message = (\n            f\"No {provider_name} API key provided. \"\n            f\"Please provide it in the config or set the {env_var_name} environment variable.\"\n        )\n\n        super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.MissingApiKeyError.__init__","title":"<code>__init__(provider_name, env_var_name)</code>","text":"<p>Initialize the exception.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Name of the provider (e.g., \"OpenAI\", \"Google\", \"Mistral\")</p> required <code>env_var_name</code> <code>str</code> <p>Name of the environment variable that should contain the API key</p> required Source code in <code>src/any_llm/exceptions.py</code> <pre><code>def __init__(self, provider_name: str, env_var_name: str) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        provider_name: Name of the provider (e.g., \"OpenAI\", \"Google\", \"Mistral\")\n        env_var_name: Name of the environment variable that should contain the API key\n\n    \"\"\"\n    self.provider_name = provider_name\n    self.env_var_name = env_var_name\n\n    message = (\n        f\"No {provider_name} API key provided. \"\n        f\"Please provide it in the config or set the {env_var_name} environment variable.\"\n    )\n\n    super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedParameterError","title":"<code>UnsupportedParameterError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an unsupported parameter is provided.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class UnsupportedParameterError(Exception):\n    \"\"\"Exception raised when an unsupported parameter is provided.\"\"\"\n\n    def __init__(self, parameter_name: str, provider_name: str, additional_message: str | None = None) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            parameter_name: Name of the parameter that was provided\n            provider_name: Name of the provider that does not support the parameter\n            additional_message: Optional additional information about the error.\n\n        \"\"\"\n        self.parameter_name = parameter_name\n        self.provider_name = provider_name\n\n        message = f\"'{parameter_name}' is not supported for {provider_name}\"\n        if additional_message is not None:\n            message = f\"{message}.\\n{additional_message}\"\n\n        super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedParameterError.__init__","title":"<code>__init__(parameter_name, provider_name, additional_message=None)</code>","text":"<p>Initialize the exception.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_name</code> <code>str</code> <p>Name of the parameter that was provided</p> required <code>provider_name</code> <code>str</code> <p>Name of the provider that does not support the parameter</p> required <code>additional_message</code> <code>str | None</code> <p>Optional additional information about the error.</p> <code>None</code> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>def __init__(self, parameter_name: str, provider_name: str, additional_message: str | None = None) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        parameter_name: Name of the parameter that was provided\n        provider_name: Name of the provider that does not support the parameter\n        additional_message: Optional additional information about the error.\n\n    \"\"\"\n    self.parameter_name = parameter_name\n    self.provider_name = provider_name\n\n    message = f\"'{parameter_name}' is not supported for {provider_name}\"\n    if additional_message is not None:\n        message = f\"{message}.\\n{additional_message}\"\n\n    super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedProviderError","title":"<code>UnsupportedProviderError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an unsupported provider is requested.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class UnsupportedProviderError(Exception):\n    \"\"\"Exception raised when an unsupported provider is requested.\"\"\"\n\n    def __init__(self, provider_key: str, supported_providers: list[str]) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            provider_key: The provider key that was requested\n            supported_providers: List of supported provider keys\n\n        \"\"\"\n        self.provider_key = provider_key\n        self.supported_providers = supported_providers\n\n        message = f\"'{provider_key}' is not a supported provider. Supported providers: {', '.join(supported_providers)}\"\n\n        super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedProviderError.__init__","title":"<code>__init__(provider_key, supported_providers)</code>","text":"<p>Initialize the exception.</p> <p>Parameters:</p> Name Type Description Default <code>provider_key</code> <code>str</code> <p>The provider key that was requested</p> required <code>supported_providers</code> <code>list[str]</code> <p>List of supported provider keys</p> required Source code in <code>src/any_llm/exceptions.py</code> <pre><code>def __init__(self, provider_key: str, supported_providers: list[str]) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        provider_key: The provider key that was requested\n        supported_providers: List of supported provider keys\n\n    \"\"\"\n    self.provider_key = provider_key\n    self.supported_providers = supported_providers\n\n    message = f\"'{provider_key}' is not a supported provider. Supported providers: {', '.join(supported_providers)}\"\n\n    super().__init__(message)\n</code></pre>"},{"location":"api/list_models/","title":"List Models","text":""},{"location":"api/list_models/#models","title":"Models","text":""},{"location":"api/list_models/#any_llm.api.list_models","title":"<code>any_llm.api.list_models(provider, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>List available models for a provider.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def list_models(\n    provider: str | LLMProvider,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Model]:\n    \"\"\"List available models for a provider.\"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return llm.list_models(**kwargs)\n</code></pre>"},{"location":"api/list_models/#any_llm.api.alist_models","title":"<code>any_llm.api.alist_models(provider, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>List available models for a provider asynchronously.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def alist_models(\n    provider: str | LLMProvider,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Model]:\n    \"\"\"List available models for a provider asynchronously.\"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm.alist_models(**kwargs)\n</code></pre>"},{"location":"api/responses/","title":"Responses","text":""},{"location":"api/responses/#responses","title":"Responses","text":"<p>Warning</p> <p>This API is experimental and subject to changes based upon our experience as we integrate additional providers. Use with caution.</p>"},{"location":"api/responses/#any_llm.api.responses","title":"<code>any_llm.api.responses(model, input_data, *, provider=None, tools=None, tool_choice=None, max_output_tokens=None, temperature=None, top_p=None, stream=None, api_key=None, api_base=None, instructions=None, max_tool_calls=None, parallel_tool_calls=None, reasoning=None, text=None, client_args=None, **kwargs)</code>","text":"<p>Create a response using the OpenAI-style Responses API.</p> <p>This follows the OpenAI Responses API shape and returns the aliased <code>any_llm.types.responses.Response</code> type. If <code>stream=True</code>, an iterator of <code>any_llm.types.responses.ResponseStreamEvent</code> items is returned.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.</p> <code>None</code> <code>input_data</code> <code>str | ResponseInputParam</code> <p>The input payload accepted by provider's Responses API. For OpenAI-compatible providers, this is typically a list mixing text, images, and tool instructions, or a dict per OpenAI spec.</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>Optional tools for tool calling (Python callables or OpenAI tool dicts)</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>max_output_tokens</code> <code>int | None</code> <p>Maximum number of output tokens to generate</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream response events</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>instructions</code> <code>str | None</code> <p>A system (or developer) message inserted into the model's context.</p> <code>None</code> <code>max_tool_calls</code> <code>int | None</code> <p>The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.</p> <code>None</code> <code>parallel_tool_calls</code> <code>int | None</code> <p>Whether to allow the model to run tool calls in parallel.</p> <code>None</code> <code>reasoning</code> <code>Any | None</code> <p>Configuration options for reasoning models.</p> <code>None</code> <code>text</code> <code>Any | None</code> <p>Configuration options for a text response from the model. Can be plain text or structured JSON data.</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response | Iterator[ResponseStreamEvent]</code> <p>Either a <code>Response</code> object (non-streaming) or an iterator of</p> <code>Response | Iterator[ResponseStreamEvent]</code> <p><code>ResponseStreamEvent</code> (streaming).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the selected provider does not support the Responses API.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def responses(\n    model: str,\n    input_data: str | ResponseInputParam,\n    *,\n    provider: str | LLMProvider | None = None,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    max_output_tokens: int | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    stream: bool | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    instructions: str | None = None,\n    max_tool_calls: int | None = None,\n    parallel_tool_calls: int | None = None,\n    reasoning: Any | None = None,\n    text: Any | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Response | Iterator[ResponseStreamEvent]:\n    \"\"\"Create a response using the OpenAI-style Responses API.\n\n    This follows the OpenAI Responses API shape and returns the aliased\n    `any_llm.types.responses.Response` type. If `stream=True`, an iterator of\n    `any_llm.types.responses.ResponseStreamEvent` items is returned.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.\n        provider: Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.\n        input_data: The input payload accepted by provider's Responses API.\n            For OpenAI-compatible providers, this is typically a list mixing\n            text, images, and tool instructions, or a dict per OpenAI spec.\n        tools: Optional tools for tool calling (Python callables or OpenAI tool dicts)\n        tool_choice: Controls which tools the model can call\n        max_output_tokens: Maximum number of output tokens to generate\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        stream: Whether to stream response events\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        instructions: A system (or developer) message inserted into the model's context.\n        max_tool_calls: The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.\n        parallel_tool_calls: Whether to allow the model to run tool calls in parallel.\n        reasoning: Configuration options for reasoning models.\n        text: Configuration options for a text response from the model. Can be plain text or structured JSON data.\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        Either a `Response` object (non-streaming) or an iterator of\n        `ResponseStreamEvent` (streaming).\n\n    Raises:\n        NotImplementedError: If the selected provider does not support the Responses API.\n\n    \"\"\"\n    all_args = locals()\n    all_args.pop(\"provider\")\n    kwargs = all_args.pop(\"kwargs\")\n\n    model = all_args.pop(\"model\")\n    if provider is None:\n        provider_key, model_id = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_id = model\n    all_args[\"model\"] = model_id\n\n    llm = AnyLLM.create(\n        provider_key,\n        api_key=all_args.pop(\"api_key\"),\n        api_base=all_args.pop(\"api_base\"),\n        **all_args.pop(\"client_args\") or {},\n    )\n    return llm.responses(**all_args, **kwargs)\n</code></pre>"},{"location":"api/responses/#any_llm.api.aresponses","title":"<code>any_llm.api.aresponses(model, input_data, *, provider=None, tools=None, tool_choice=None, max_output_tokens=None, temperature=None, top_p=None, stream=None, api_key=None, api_base=None, instructions=None, max_tool_calls=None, parallel_tool_calls=None, reasoning=None, text=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a response using the OpenAI-style Responses API.</p> <p>This follows the OpenAI Responses API shape and returns the aliased <code>any_llm.types.responses.Response</code> type. If <code>stream=True</code>, an iterator of <code>any_llm.types.responses.ResponseStreamEvent</code> items is returned.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.</p> <code>None</code> <code>input_data</code> <code>str | ResponseInputParam</code> <p>The input payload accepted by provider's Responses API. For OpenAI-compatible providers, this is typically a list mixing text, images, and tool instructions, or a dict per OpenAI spec.</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>Optional tools for tool calling (Python callables or OpenAI tool dicts)</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>max_output_tokens</code> <code>int | None</code> <p>Maximum number of output tokens to generate</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream response events</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>instructions</code> <code>str | None</code> <p>A system (or developer) message inserted into the model's context.</p> <code>None</code> <code>max_tool_calls</code> <code>int | None</code> <p>The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.</p> <code>None</code> <code>parallel_tool_calls</code> <code>int | None</code> <p>Whether to allow the model to run tool calls in parallel.</p> <code>None</code> <code>reasoning</code> <code>Any | None</code> <p>Configuration options for reasoning models.</p> <code>None</code> <code>text</code> <code>Any | None</code> <p>Configuration options for a text response from the model. Can be plain text or structured JSON data.</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response | AsyncIterator[ResponseStreamEvent]</code> <p>Either a <code>Response</code> object (non-streaming) or an iterator of</p> <code>Response | AsyncIterator[ResponseStreamEvent]</code> <p><code>ResponseStreamEvent</code> (streaming).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the selected provider does not support the Responses API.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def aresponses(\n    model: str,\n    input_data: str | ResponseInputParam,\n    *,\n    provider: str | LLMProvider | None = None,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    max_output_tokens: int | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    stream: bool | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    instructions: str | None = None,\n    max_tool_calls: int | None = None,\n    parallel_tool_calls: int | None = None,\n    reasoning: Any | None = None,\n    text: Any | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Response | AsyncIterator[ResponseStreamEvent]:\n    \"\"\"Create a response using the OpenAI-style Responses API.\n\n    This follows the OpenAI Responses API shape and returns the aliased\n    `any_llm.types.responses.Response` type. If `stream=True`, an iterator of\n    `any_llm.types.responses.ResponseStreamEvent` items is returned.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.\n        provider: Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.\n        input_data: The input payload accepted by provider's Responses API.\n            For OpenAI-compatible providers, this is typically a list mixing\n            text, images, and tool instructions, or a dict per OpenAI spec.\n        tools: Optional tools for tool calling (Python callables or OpenAI tool dicts)\n        tool_choice: Controls which tools the model can call\n        max_output_tokens: Maximum number of output tokens to generate\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        stream: Whether to stream response events\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        instructions: A system (or developer) message inserted into the model's context.\n        max_tool_calls: The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.\n        parallel_tool_calls: Whether to allow the model to run tool calls in parallel.\n        reasoning: Configuration options for reasoning models.\n        text: Configuration options for a text response from the model. Can be plain text or structured JSON data.\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        Either a `Response` object (non-streaming) or an iterator of\n        `ResponseStreamEvent` (streaming).\n\n    Raises:\n        NotImplementedError: If the selected provider does not support the Responses API.\n\n    \"\"\"\n    all_args = locals()\n    all_args.pop(\"provider\")\n    kwargs = all_args.pop(\"kwargs\")\n\n    model = all_args.pop(\"model\")\n    if provider is None:\n        provider_key, model_id = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_id = model\n    all_args[\"model\"] = model_id\n\n    llm = AnyLLM.create(\n        provider_key,\n        api_key=all_args.pop(\"api_key\"),\n        api_base=all_args.pop(\"api_base\"),\n        **all_args.pop(\"client_args\") or {},\n    )\n    return await llm.aresponses(**all_args, **kwargs)\n</code></pre>"},{"location":"api/types/batch/","title":"Batch","text":""},{"location":"api/types/batch/#batch-types","title":"Batch Types","text":"<p>Data models and types for batch operations.</p>"},{"location":"api/types/batch/#any_llm.types.batch","title":"<code>any_llm.types.batch</code>","text":""},{"location":"api/types/completion/","title":"Completion","text":""},{"location":"api/types/completion/#completion-types","title":"Completion Types","text":"<p>Data models and types for completion operations.</p>"},{"location":"api/types/completion/#any_llm.types.completion","title":"<code>any_llm.types.completion</code>","text":""},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams","title":"<code>CompletionParams</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Normalized parameters for chat completions.</p> <p>This model is used internally to pass structured parameters from the public API layer to provider implementations, avoiding very long function signatures while keeping type safety.</p> Source code in <code>src/any_llm/types/completion.py</code> <pre><code>class CompletionParams(BaseModel):\n    \"\"\"Normalized parameters for chat completions.\n\n    This model is used internally to pass structured parameters from the public\n    API layer to provider implementations, avoiding very long function\n    signatures while keeping type safety.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    model_id: str\n    \"\"\"Model identifier (e.g., 'mistral-small-latest')\"\"\"\n\n    messages: list[dict[str, Any]]\n    \"\"\"List of messages for the conversation\"\"\"\n\n    @field_validator(\"messages\")\n    def check_messages_not_empty(cls, v: list[dict[str, Any]]) -&gt; list[dict[str, Any]]:  # noqa: N805\n        if not v:\n            msg = \"The `messages` list cannot be empty.\"\n            raise ValueError(msg)\n        return v\n\n    tools: list[dict[str, Any] | Any] | None = None\n    \"\"\"List of tools for tool calling. Should be converted to OpenAI tool format dicts\"\"\"\n\n    tool_choice: str | dict[str, Any] | None = None\n    \"\"\"Controls which tools the model can call\"\"\"\n\n    temperature: float | None = None\n    \"\"\"Controls randomness in the response (0.0 to 2.0)\"\"\"\n\n    top_p: float | None = None\n    \"\"\"Controls diversity via nucleus sampling (0.0 to 1.0)\"\"\"\n\n    max_tokens: int | None = None\n    \"\"\"Maximum number of tokens to generate\"\"\"\n\n    response_format: dict[str, Any] | type[BaseModel] | None = None\n    \"\"\"Format specification for the response\"\"\"\n\n    stream: bool | None = None\n    \"\"\"Whether to stream the response\"\"\"\n\n    n: int | None = None\n    \"\"\"Number of completions to generate\"\"\"\n\n    stop: str | list[str] | None = None\n    \"\"\"Stop sequences for generation\"\"\"\n\n    presence_penalty: float | None = None\n    \"\"\"Penalize new tokens based on presence in text\"\"\"\n\n    frequency_penalty: float | None = None\n    \"\"\"Penalize new tokens based on frequency in text\"\"\"\n\n    seed: int | None = None\n    \"\"\"Random seed for reproducible results\"\"\"\n\n    user: str | None = None\n    \"\"\"Unique identifier for the end user\"\"\"\n\n    parallel_tool_calls: bool | None = None\n    \"\"\"Whether to allow parallel tool calls\"\"\"\n\n    logprobs: bool | None = None\n    \"\"\"Include token-level log probabilities in the response\"\"\"\n\n    top_logprobs: int | None = None\n    \"\"\"Number of top alternatives to return when logprobs are requested\"\"\"\n\n    logit_bias: dict[str, float] | None = None\n    \"\"\"Bias the likelihood of specified tokens during generation\"\"\"\n\n    stream_options: dict[str, Any] | None = None\n    \"\"\"Additional options controlling streaming behavior\"\"\"\n\n    max_completion_tokens: int | None = None\n    \"\"\"Maximum number of tokens for the completion (provider-dependent)\"\"\"\n\n    reasoning_effort: Literal[\"minimal\", \"low\", \"medium\", \"high\", \"auto\"] | None = \"auto\"\n    \"\"\"Reasoning effort level for models that support it. \"auto\" will map to each provider's default.\"\"\"\n</code></pre>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.frequency_penalty","title":"<code>frequency_penalty = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Penalize new tokens based on frequency in text</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.logit_bias","title":"<code>logit_bias = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Bias the likelihood of specified tokens during generation</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.logprobs","title":"<code>logprobs = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Include token-level log probabilities in the response</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.max_completion_tokens","title":"<code>max_completion_tokens = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of tokens for the completion (provider-dependent)</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.max_tokens","title":"<code>max_tokens = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of tokens to generate</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.messages","title":"<code>messages</code>  <code>instance-attribute</code>","text":"<p>List of messages for the conversation</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.model_id","title":"<code>model_id</code>  <code>instance-attribute</code>","text":"<p>Model identifier (e.g., 'mistral-small-latest')</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.n","title":"<code>n = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of completions to generate</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.parallel_tool_calls","title":"<code>parallel_tool_calls = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to allow parallel tool calls</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.presence_penalty","title":"<code>presence_penalty = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Penalize new tokens based on presence in text</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.reasoning_effort","title":"<code>reasoning_effort = 'auto'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Reasoning effort level for models that support it. \"auto\" will map to each provider's default.</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.response_format","title":"<code>response_format = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Format specification for the response</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.seed","title":"<code>seed = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random seed for reproducible results</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.stop","title":"<code>stop = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Stop sequences for generation</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.stream","title":"<code>stream = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to stream the response</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.stream_options","title":"<code>stream_options = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Additional options controlling streaming behavior</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.temperature","title":"<code>temperature = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls randomness in the response (0.0 to 2.0)</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.tool_choice","title":"<code>tool_choice = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls which tools the model can call</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.tools","title":"<code>tools = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of tools for tool calling. Should be converted to OpenAI tool format dicts</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.top_logprobs","title":"<code>top_logprobs = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of top alternatives to return when logprobs are requested</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.top_p","title":"<code>top_p = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls diversity via nucleus sampling (0.0 to 1.0)</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.user","title":"<code>user = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Unique identifier for the end user</p>"},{"location":"api/types/model/","title":"Model","text":""},{"location":"api/types/model/#model-types","title":"Model Types","text":"<p>Data models and types for model operations.</p>"},{"location":"api/types/model/#any_llm.types.model","title":"<code>any_llm.types.model</code>","text":""},{"location":"api/types/provider/","title":"Provider","text":""},{"location":"api/types/provider/#provider-types","title":"Provider Types","text":"<p>Data models and types for provider operations.</p>"},{"location":"api/types/provider/#any_llm.types.provider","title":"<code>any_llm.types.provider</code>","text":""},{"location":"api/types/responses/","title":"Responses","text":""},{"location":"api/types/responses/#response-types","title":"Response Types","text":"<p>Data models and types for API responses.</p>"},{"location":"api/types/responses/#any_llm.types.responses","title":"<code>any_llm.types.responses</code>","text":""},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams","title":"<code>ResponsesParams</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Normalized parameters for responses API.</p> <p>This model is used internally to pass structured parameters from the public API layer to provider implementations, avoiding very long function signatures while keeping type safety.</p> Source code in <code>src/any_llm/types/responses.py</code> <pre><code>class ResponsesParams(BaseModel):\n    \"\"\"Normalized parameters for responses API.\n\n    This model is used internally to pass structured parameters from the public\n    API layer to provider implementations, avoiding very long function\n    signatures while keeping type safety.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    model: str\n    \"\"\"Model identifier (e.g., 'mistral-small-latest')\"\"\"\n\n    input: str | ResponseInputParam\n    \"\"\"The input payload accepted by provider's Responses API.\n        For OpenAI-compatible providers, this is typically a list mixing\n        text, images, and tool instructions, or a dict per OpenAI spec.\n    \"\"\"\n\n    instructions: str | None = None\n\n    max_tool_calls: int | None = None\n\n    text: Any | None = None\n\n    tools: list[dict[str, Any]] | None = None\n    \"\"\"List of tools for tool calling. Should be converted to OpenAI tool format dicts\"\"\"\n\n    tool_choice: str | dict[str, Any] | None = None\n    \"\"\"Controls which tools the model can call\"\"\"\n\n    temperature: float | None = None\n    \"\"\"Controls randomness in the response (0.0 to 2.0)\"\"\"\n\n    top_p: float | None = None\n    \"\"\"Controls diversity via nucleus sampling (0.0 to 1.0)\"\"\"\n\n    max_output_tokens: int | None = None\n    \"\"\"Maximum number of tokens to generate\"\"\"\n\n    response_format: dict[str, Any] | type[BaseModel] | None = None\n    \"\"\"Format specification for the response\"\"\"\n\n    stream: bool | None = None\n    \"\"\"Whether to stream the response\"\"\"\n\n    parallel_tool_calls: bool | None = None\n    \"\"\"Whether to allow parallel tool calls\"\"\"\n\n    top_logprobs: int | None = None\n    \"\"\"Number of top alternatives to return when logprobs are requested\"\"\"\n\n    stream_options: dict[str, Any] | None = None\n    \"\"\"Additional options controlling streaming behavior\"\"\"\n\n    reasoning: dict[str, Any] | None = None\n    \"\"\"Configuration options for reasoning models.\"\"\"\n</code></pre>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.input","title":"<code>input</code>  <code>instance-attribute</code>","text":"<p>The input payload accepted by provider's Responses API. For OpenAI-compatible providers, this is typically a list mixing text, images, and tool instructions, or a dict per OpenAI spec.</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.max_output_tokens","title":"<code>max_output_tokens = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of tokens to generate</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.model","title":"<code>model</code>  <code>instance-attribute</code>","text":"<p>Model identifier (e.g., 'mistral-small-latest')</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.parallel_tool_calls","title":"<code>parallel_tool_calls = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to allow parallel tool calls</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.reasoning","title":"<code>reasoning = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Configuration options for reasoning models.</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.response_format","title":"<code>response_format = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Format specification for the response</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.stream","title":"<code>stream = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to stream the response</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.stream_options","title":"<code>stream_options = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Additional options controlling streaming behavior</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.temperature","title":"<code>temperature = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls randomness in the response (0.0 to 2.0)</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.tool_choice","title":"<code>tool_choice = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls which tools the model can call</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.tools","title":"<code>tools = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of tools for tool calling. Should be converted to OpenAI tool format dicts</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.top_logprobs","title":"<code>top_logprobs = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of top alternatives to return when logprobs are requested</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.top_p","title":"<code>top_p = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls diversity via nucleus sampling (0.0 to 1.0)</p>"},{"location":"cookbooks/any_llm_getting_started/","title":"Getting Started with Any-LLM","text":"<p>Any-LLM is a unified interface that lets you work with language models from any provider using a consistent API. Whether you're using OpenAI, Anthropic, Google, local models, or open-source alternatives, any-llm makes it easy to switch between them without changing your code.</p> In\u00a0[\u00a0]: Copied! <pre>%pip install any-llm-sdk[all] nest-asyncio -q\n\n# nest_asyncio allows us to use 'await' directly in Jupyter notebooks\n# This is needed because any-llm uses async functions for API calls\nimport nest_asyncio\n\nnest_asyncio.apply()\n</pre> %pip install any-llm-sdk[all] nest-asyncio -q  # nest_asyncio allows us to use 'await' directly in Jupyter notebooks # This is needed because any-llm uses async functions for API calls import nest_asyncio  nest_asyncio.apply() In\u00a0[\u00a0]: Copied! <pre>import os\nfrom getpass import getpass\n\n\ndef setup_api_key(key_name: str, provider: str) -&gt; None:\n    \"\"\"Set up API key for the specified provider.\"\"\"\n    if key_name not in os.environ:\n        print(f\"\ud83d\udd11 {key_name} not found in environment\")\n        api_key = getpass(f\"Enter your {provider} API key (or press Enter to skip): \")\n        if api_key:\n            os.environ[key_name] = api_key\n            print(f\"\u2705 {key_name} set for this session\")\n        else:\n            print(f\"\u23ed\ufe0f  Skipping {provider}\")\n    else:\n        print(f\"\u2705 {key_name} found in environment\")\n\n\n# Set up keys for different providers\nprint(\"Setting up API keys...\\n\")\nsetup_api_key(\"OPENAI_API_KEY\", \"OpenAI\")\nsetup_api_key(\"ANTHROPIC_API_KEY\", \"Anthropic\")\n\n#  You could add more using :\n# setup_api_key(\"GOOGLE_API_KEY\", \"Google\")\n# setup_api_key(\"MISTRAL_API_KEY\", \"Mistral\")\n</pre> import os from getpass import getpass   def setup_api_key(key_name: str, provider: str) -&gt; None:     \"\"\"Set up API key for the specified provider.\"\"\"     if key_name not in os.environ:         print(f\"\ud83d\udd11 {key_name} not found in environment\")         api_key = getpass(f\"Enter your {provider} API key (or press Enter to skip): \")         if api_key:             os.environ[key_name] = api_key             print(f\"\u2705 {key_name} set for this session\")         else:             print(f\"\u23ed\ufe0f  Skipping {provider}\")     else:         print(f\"\u2705 {key_name} found in environment\")   # Set up keys for different providers print(\"Setting up API keys...\\n\") setup_api_key(\"OPENAI_API_KEY\", \"OpenAI\") setup_api_key(\"ANTHROPIC_API_KEY\", \"Anthropic\")  #  You could add more using : # setup_api_key(\"GOOGLE_API_KEY\", \"Google\") # setup_api_key(\"MISTRAL_API_KEY\", \"Mistral\") In\u00a0[\u00a0]: Copied! <pre>from any_llm import AnyLLM, LLMProvider\n\nfor provider in [LLMProvider.OPENAI, LLMProvider.ANTHROPIC]:\n    client = AnyLLM.create(provider=provider)\n    models = client.list_models()\n    print(f\"Provider: {provider}\")\n    print(\", \".join([model.id for model in models]))\n    print()\n</pre> from any_llm import AnyLLM, LLMProvider  for provider in [LLMProvider.OPENAI, LLMProvider.ANTHROPIC]:     client = AnyLLM.create(provider=provider)     models = client.list_models()     print(f\"Provider: {provider}\")     print(\", \".join([model.id for model in models]))     print() In\u00a0[\u00a0]: Copied! <pre>from any_llm import acompletion\nfrom any_llm.types.completion import ChatCompletion\n\nprompt = \"Write a Haiku on the solar system.\"\n\n# OpenAI\nmodel = \"openai:gpt-4o-mini\"\nresult = await acompletion(\n    model=model,\n    messages=[\n        {\"role\": \"user\", \"content\": prompt},\n    ],\n)\nassert isinstance(result, ChatCompletion)\n\nprint(f\"Model: {result.model}\")\nprint(f\"Response:\\n{result.choices[0].message.content}\\n\")\n\n# Anthropic\nmodel = \"anthropic:claude-haiku-4-5-20251001\"\nresult = await acompletion(\n    model=model,\n    messages=[\n        {\"role\": \"user\", \"content\": prompt},\n    ],\n)\n\nassert isinstance(result, ChatCompletion)\n\nprint(f\"Model: {result.model}\")\nprint(f\"Response:\\n{result.choices[0].message.content}\")\n</pre> from any_llm import acompletion from any_llm.types.completion import ChatCompletion  prompt = \"Write a Haiku on the solar system.\"  # OpenAI model = \"openai:gpt-4o-mini\" result = await acompletion(     model=model,     messages=[         {\"role\": \"user\", \"content\": prompt},     ], ) assert isinstance(result, ChatCompletion)  print(f\"Model: {result.model}\") print(f\"Response:\\n{result.choices[0].message.content}\\n\")  # Anthropic model = \"anthropic:claude-haiku-4-5-20251001\" result = await acompletion(     model=model,     messages=[         {\"role\": \"user\", \"content\": prompt},     ], )  assert isinstance(result, ChatCompletion)  print(f\"Model: {result.model}\") print(f\"Response:\\n{result.choices[0].message.content}\")"},{"location":"cookbooks/any_llm_getting_started/#getting-started-with-any-llm","title":"Getting Started with Any-LLM\u00b6","text":""},{"location":"cookbooks/any_llm_getting_started/#why-any-llm","title":"Why Any-LLM?\u00b6","text":"<ul> <li>Provider Agnostic: One API for all LLM providers</li> <li>Easy Switching: Change models with a single line</li> <li>Cost Comparison: Compare costs across providers</li> <li>Streaming Support: Real-time responses from any model</li> <li>Type Safe: Full TypeScript/Python type support</li> </ul>"},{"location":"cookbooks/any_llm_getting_started/#installation","title":"Installation\u00b6","text":""},{"location":"cookbooks/any_llm_getting_started/#setting-up-api-keys","title":"Setting Up API Keys\u00b6","text":"<p>Different providers require different API keys. Let's set them up properly:</p>"},{"location":"cookbooks/any_llm_getting_started/#list-models-across-providers","title":"List Models Across Providers\u00b6","text":"<p><code>any_llm</code> can list all available models for an LLM provider - in this case, we are listing out models supported by OpenAI and Anthropic.</p>"},{"location":"cookbooks/any_llm_getting_started/#expected-output","title":"Expected output\u00b6","text":"<p>Provider: openai gpt-4o-mini, gpt-4-0613, gpt-4, gpt-3.5-turbo, gpt-5-search-api-2025-10-14, gpt-realtime-mini, gpt-realtime-mini-2025-10-06, sora-2, sora-2-pro, davinci-002, babbage-002, gpt-3.5-turbo-instruct, gpt-3.5-turbo-instruct-0914...</p> <p>Provider: anthropic claude-haiku-4-5-20251001, claude-sonnet-4-5-20250929, claude-opus-4-1-20250805, claude-opus-4-20250514, claude-sonnet-4-20250514, claude-3-7-sonnet-20250219, claude-3-5-haiku-20241022, claude-3-haiku-20240307</p>"},{"location":"cookbooks/any_llm_getting_started/#generate-text","title":"Generate Text\u00b6","text":"<p>Let's use one model from each provider to generate text for the same prompt.</p>"},{"location":"cookbooks/any_llm_getting_started/#expected-output","title":"Expected Output\u00b6","text":"<p>Note: The haiku content will be different each time since it's generated by the LLM. This example shows the output format.</p> <p>Model: gpt-4o-mini-2024-07-18 Response:</p> <p>Planets spin and dance, In the vast cosmic embrace, Stars whisper their tales.</p> <p>Model: claude-haiku-4-5-20251001 Response:</p> <p>Eight worlds circle round, Sun's gravity holds them close\u2014 Dance through endless void.</p>"},{"location":"gateway/api-reference/","title":"API Reference","text":""},{"location":"gateway/authentication/","title":"Authentication","text":"<p>The any-llm Gateway supports two main authentication patterns for making completion requests.</p>"},{"location":"gateway/authentication/#direct-master-key-authentication","title":"Direct Master Key Authentication","text":"<p>Use the master key directly and specify which user is making the request.</p>"},{"location":"gateway/authentication/#creating-a-user","title":"Creating a User","text":"<pre><code>curl -X POST http://localhost:8000/v1/users \\\n  -H \"X-AnyLLM-Key: Bearer your-secure-master-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"user_id\": \"user-123\", \"alias\": \"Alice\"}'\n</code></pre>"},{"location":"gateway/authentication/#making-requests-with-master-key","title":"Making Requests with Master Key","text":"<pre><code>curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"X-AnyLLM-Key: Bearer your-secure-master-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai:gpt-4o-mini\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n    \"user\": \"user-123\"\n  }'\n</code></pre>"},{"location":"gateway/authentication/#virtual-api-keys","title":"Virtual API Keys","text":"<p>Virtual API keys provide a more secure way to authenticate requests without exposing the master key.</p>"},{"location":"gateway/authentication/#creating-a-virtual-api-key","title":"Creating a Virtual API Key","text":"<pre><code>curl -X POST http://localhost:8000/v1/keys \\\n  -H \"X-AnyLLM-Key: Bearer your-secure-master-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"key_name\": \"mobile-app\"}'\n</code></pre> <p>Response: <pre><code>{\n  \"id\": \"abc-123\",\n  \"key\": \"gw-...\",\n  \"key_name\": \"mobile-app\",\n  \"created_at\": \"2025-10-20T10:00:00\",\n  \"expires_at\": null,\n  \"is_active\": true,\n  \"metadata\": {}\n}\n</code></pre></p>"},{"location":"gateway/authentication/#using-virtual-api-keys","title":"Using Virtual API Keys","text":"<pre><code>curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"X-AnyLLM-Key: Bearer gw-...\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"openai:gpt-5-mini\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n</code></pre> <p>Usage is automatically tracked under the virtual user associated with the virtual key.</p>"},{"location":"gateway/budget-management/","title":"Budget Management","text":"<p>Budgets provide shared spending limits that can be assigned to multiple users. This allows you to create budget tiers (like \"Free\", \"Pro\", \"Enterprise\") and enforce spending limits across groups of users.</p>"},{"location":"gateway/budget-management/#creating-a-budget","title":"Creating a Budget","text":"<pre><code># Create a budget with a $10.00 spending limit and monthly resets (30 days = 2592000 seconds)\ncurl -X POST http://localhost:8000/v1/budgets \\\n  -H \"X-AnyLLM-Key: Bearer your-secure-master-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"max_budget\": 10.0,\n    \"budget_duration_sec\": 2592000\n  }'\n</code></pre> <p>Response: <pre><code>{\n  \"budget_id\": \"abc-123\",\n  \"max_budget\": 10.0,\n  \"budget_duration_sec\": 2592000,\n  \"created_at\": \"2025-10-22T10:00:00Z\",\n  \"updated_at\": \"2025-10-22T10:00:00Z\"\n}\n</code></pre></p>"},{"location":"gateway/budget-management/#assigning-budgets-to-users","title":"Assigning Budgets to Users","text":"<p>When creating or updating a user, specify the <code>budget_id</code>:</p> <p>Warning: If you don't create and set a budget, budget is unlimited</p> <pre><code># Create a user with a budget\ncurl -X POST http://localhost:8000/v1/users \\\n  -H \"X-AnyLLM-Key: Bearer your-secure-master-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"user_id\": \"user-456\",\n    \"alias\": \"Bob\",\n    \"budget_id\": \"abc-123\"\n  }'\n\n# Update an existing user's budget\ncurl -X PATCH http://localhost:8000/v1/users/user-123 \\\n  -H \"X-AnyLLM-Key: Bearer your-secure-master-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"budget_id\": \"abc-123\"}'\n</code></pre>"},{"location":"gateway/budget-management/#per-user-budget-resets","title":"Per-User Budget Resets","text":"<p>Budget resets are per-user, not global. Each user tracks their own budget period based on when they were assigned the budget.</p> <p>Example: 1. Create a budget with <code>budget_duration_sec: 604800</code> (1 week) 2. Assign User A to the budget on Monday 3. Assign User B to the budget on Tuesday 4. User A's budget resets every Monday 5. User B's budget resets every Tuesday</p> <p>This allows you to create budget tiers (like \"Free\", \"Pro\", \"Enterprise\") without worrying about all users resetting at the same time.</p>"},{"location":"gateway/budget-management/#automatic-reset-behavior","title":"Automatic Reset Behavior","text":"<p>Budget resets happen automatically using a \"lazy reset\" approach: - When a user makes a request, the system checks if their <code>next_budget_reset_at</code> has passed - If yes, the user's <code>spend</code> is reset to $0.00 and a new reset date is calculated - A log entry is created in <code>budget_reset_logs</code> for audit purposes - The request then proceeds normally</p>"},{"location":"gateway/configuration/","title":"Configuration","text":""},{"location":"gateway/configuration/#option-1-config-file","title":"Option 1: Config File","text":"<p>Create a <code>config.yaml</code>:</p> <pre><code>database_url: \"postgresql://gateway:gateway@localhost:5432/gateway_db\"\nmaster_key: \"your-secure-master-key\"\n\nproviders:\n  openai:\n    api_key: \"${OPENAI_API_KEY}\"\n  gemini:\n    api_key: \"${GEMINI_API_KEY}\"\n  vertexai:\n    credentials: \"/path/to/service_account.json\"\n    project: \"your-gcp-project-id\"\n    location: \"us-central1\"\n\npricing:\n  openai:gpt-3.5-turbo:\n    input_price_per_million: 0.5\n    output_price_per_million: 1.5\n</code></pre> <p>Start with config file: <pre><code>any-llm-gateway serve --config config.yaml\n</code></pre></p>"},{"location":"gateway/configuration/#option-2-environment-variables","title":"Option 2: Environment Variables","text":"<pre><code>export DATABASE_URL=\"postgresql://gateway:gateway@localhost:5432/gateway_db\"\nexport GATEWAY_MASTER_KEY=\"your-secure-master-key\"\nexport GATEWAY_HOST=\"0.0.0.0\"\nexport GATEWAY_PORT=8000\n\nany-llm-gateway serve\n</code></pre>"},{"location":"gateway/configuration/#model-pricing-configuration","title":"Model Pricing Configuration","text":"<p>Configure model pricing in your config file to automatically track costs. Pricing can be set via config file or dynamically via the API.</p>"},{"location":"gateway/configuration/#config-file-pricing","title":"Config File Pricing","text":"<p>Add pricing for models in your config file using the format <code>provider:model</code>:</p> <pre><code>pricing:\n  openai:gpt-3.5-turbo:\n    input_price_per_million: 0.5\n    output_price_per_million: 1.5\n</code></pre> <p>Important notes: - Database pricing takes precedence - config only sets initial values - If pricing for the model already exists in the database, config values are ignored (with a warning logged)</p>"},{"location":"gateway/overview/","title":"Gateway Overview","text":""},{"location":"gateway/overview/#what-is-any-llm-gateway","title":"What is any-llm gateway?","text":"<p>any-llm gateway is a FastAPI-based proxy server that adds production-grade budget enforcement, API key management, and usage analytics on top of any-llm's multi-provider foundation. It sits between your applications and LLM providers, giving you complete control over costs, access, and observability.</p>"},{"location":"gateway/overview/#why-use-the-gateway","title":"Why use the gateway?","text":"<p>Managing LLM costs and access at scale is challenging. Give users unrestricted access and you risk runaway costs. Lock it down too much and you slow down innovation. any-llm gateway solves this by providing:</p> <ul> <li>Cost Control: Set budgets that automatically enforce or track spending limits</li> <li>Access Management: Issue, revoke, and monitor API keys generated for user access without exposing provider credentials</li> <li>Complete Visibility: Track every request with full token counts, costs, and metadata</li> <li>Production-Ready: Deploy with Docker and Postgres, Kubernetes-ready</li> </ul>"},{"location":"gateway/overview/#how-it-works","title":"How it works","text":"<p>The gateway exposes an OpenAI-compatible Completions API that works with any supported provider. Your applications connect to the gateway instead of directly to LLM providers, and the gateway handles:</p> <ul> <li>Authentication: Validates requests using master keys or virtual API keys</li> <li>Budget Enforcement: Checks spending limits before forwarding requests</li> <li>Provider Routing: Routes requests to the appropriate LLM provider using the <code>provider:model</code> format (e.g., <code>openai:gpt-4o-mini</code>, <code>anthropic:claude-3-5-sonnet-20241022</code>)</li> <li>Usage Tracking: Logs all requests with token counts and costs</li> <li>Streaming Support: Handles streaming responses with automatic token tracking</li> </ul>"},{"location":"gateway/overview/#key-features","title":"Key Features","text":""},{"location":"gateway/overview/#smart-budget-management","title":"Smart Budget Management","text":"<p>Create shared budget tiers with automatic daily, weekly, or monthly resets. Budgets can be:</p> <ul> <li>Shared across multiple users - Perfect for team or organization-wide limits</li> <li>Automatically enforced - Requests are rejected when budgets are exceeded</li> <li>Tracking-only mode - Monitor spending without blocking requests</li> <li>Auto-resetting - No manual intervention required for recurring budgets</li> </ul> <p>Set up your first budget \u2192</p>"},{"location":"gateway/overview/#flexible-api-key-system","title":"Flexible API Key System","text":"<p>Choose between two authentication patterns:</p> <p>Master Key Authentication * Ideal for trusted services and internal tools * Full access to all gateway features</p> <p>Virtual API Keys * Create scoped keys with fine-grained control * Set expiration dates for time-limited access * Associate with users for spend tracking * Add custom metadata for tracking * Activate, deactivate, or revoke on demand</p> <p>Learn more about authentication \u2192</p>"},{"location":"gateway/overview/#complete-usage-analytics","title":"Complete Usage Analytics","text":"<p>Every request is logged with comprehensive details:</p> <ul> <li>Full token counts (prompt, completion, total)</li> <li>Per-request costs based on admin-configured per-token pricing</li> <li>Request metadata and timestamps</li> <li>User and API key attribution</li> </ul> <p>Track spending per user, view detailed usage history, and get the observability you need for cost attribution and chargebacks.</p>"},{"location":"gateway/overview/#production-ready-deployment","title":"Production-Ready Deployment","text":"<ul> <li>Quick Start: Deploy with Docker in minutes</li> <li>Flexible Configuration: Configure via YAML or environment variables</li> <li>Database: Designed for PostgreSQL</li> <li>Kubernetes Ready: Built-in liveness and readiness probes</li> </ul>"},{"location":"gateway/overview/#performance-impact","title":"Performance Impact","text":"<p>The gateway adds minimal latency (&lt;50ms) to requests while providing complete observability.</p>"},{"location":"gateway/overview/#getting-started","title":"Getting Started","text":"<p>For comprehensive setup instructions, see the Quick Start Guide.</p>"},{"location":"gateway/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start - Deploy and configure your first gateway</li> <li>Authentication - Set up master keys and virtual API keys</li> <li>Budget Management - Configure spending limits and tracking</li> <li>Configuration - Learn about all configuration options</li> <li>API Reference - Explore the complete API</li> </ul>"},{"location":"gateway/quickstart/","title":"Quick Start","text":""},{"location":"gateway/quickstart/#run-from-docker-image","title":"Run from Docker Image","text":"<pre><code>docker run \\\n-e GATEWAY_MASTER_KEY=\"your-secure-master-key\" \\\n-e OPENAI_API_KEY=\"your-api-key\" \\\n-p 8000:8000 \\\nghcr.io/mozilla-ai/any-llm/gateway:latest\n</code></pre>"},{"location":"gateway/quickstart/#local-development","title":"Local development","text":""},{"location":"gateway/quickstart/#option-1-docker-compose","title":"Option 1: Docker compose","text":"<p>First, create a <code>config.yaml</code> file with your configuration, using config.example.yaml as a template.</p> <p>Then run the Docker containers:</p> <pre><code>docker-compose -f docker/docker-compose.yml up -d --build\n\n# Tail the logs\ndocker-compose -f docker/docker-compose.yml logs -f\n</code></pre> <p>This will run any-llm-gateway using the credentials and configuration specified in <code>config.yaml</code>.</p>"},{"location":"gateway/quickstart/#option-2-cli","title":"Option 2: CLI","text":"<p>In order for this to work, you will need to have a Postgres DB running. <pre><code>uv venv --python=3.13\nsource .venv/bin/activate\nuv sync --all-extras -U\n</code></pre></p> <pre><code>export GATEWAY_MASTER_KEY=\"your-secure-master-key\"\nexport DATABASE_URL=\"postgresql://postgres:postgres@localhost:5432/any_llm_gateway\"\nexport OPENAI_API_KEY=\"your-api-key\" # Or GEMINI_API_KEY etc\n\nany-llm-gateway serve # Or, you can put the env vars in a config.yaml file and run serve with --config path/to/yaml\n</code></pre>"},{"location":"gateway/quickstart/#basic-usage","title":"Basic Usage","text":"<p>The gateway supports two authentication patterns for making completion requests:</p>"},{"location":"gateway/quickstart/#option-1-direct-master-key-authentication","title":"Option 1: Direct Master Key Authentication","text":"<p>First, create a user.</p> <pre><code>curl -X POST http://localhost:8000/v1/users \\\n  -H \"X-AnyLLM-Key: Bearer your-secure-master-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"user_id\": \"user-123\", \"alias\": \"Alice\"}'\n</code></pre> <p>Use the master key directly and specify which user is making the request.</p> <pre><code>curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"X-AnyLLM-Key: Bearer your-secure-master-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"openai:gpt-4o-mini\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n    \"user\": \"user-123\"\n  }'\n</code></pre>"},{"location":"gateway/quickstart/#option-2-virtual-api-keys","title":"Option 2: Virtual API Keys","text":"<p>Create a virtual API key (you can optionally pass in a user_id too if you want the key linked to a user)</p> <pre><code>curl -X POST http://localhost:8000/v1/keys \\\n  -H \"X-AnyLLM-Key: Bearer your-secure-master-key\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"key_name\": \"mobile-app\"}'\n</code></pre> <p>Now you can use that new api key and don't need to pass in the user field.</p> <pre><code>curl -X POST http://localhost:8000/v1/chat/completions \\\n  -H \"X-AnyLLM-Key: Bearer gw-...\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"model\": \"openai:gpt-5-mini\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]}'\n</code></pre> <p>Usage is automatically tracked under the virtual user associated with the virtual key.</p>"},{"location":"gateway/troubleshooting/","title":"Troubleshooting","text":""},{"location":"gateway/troubleshooting/#database-connection-errors","title":"Database connection errors","text":"<p>Make sure the database URL is correct and the database is accessible:</p> <pre><code>python -c \"from sqlalchemy import create_engine; engine = create_engine('postgresql://user:pass@host/db'); print('OK')\"\n</code></pre>"},{"location":"gateway/troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"gateway/troubleshooting/#authentication-errors","title":"Authentication Errors","text":"<ul> <li>Ensure you're using the correct master key format: <code>Bearer your-secure-master-key</code></li> <li>Check that the <code>X-AnyLLM-Key</code> header is properly set</li> <li>Verify that virtual API keys are active and not expired</li> </ul>"},{"location":"gateway/troubleshooting/#configuration-issues","title":"Configuration Issues","text":"<ul> <li>Verify your <code>config.yaml</code> file is properly formatted</li> <li>Check that environment variables are set correctly</li> <li>Ensure provider API keys are valid and have proper permissions</li> </ul>"},{"location":"gateway/troubleshooting/#budget-enforcement","title":"Budget Enforcement","text":"<ul> <li>Check that budgets are properly assigned to users</li> <li>Verify budget limits are set correctly</li> <li>Monitor user spending to ensure limits are being enforced</li> </ul>"},{"location":"gateway/troubleshooting/#getting-help","title":"Getting Help","text":"<ul> <li>Check the logs for detailed error messages</li> <li>Verify your configuration matches the examples in the documentation</li> <li>Ensure all required environment variables are set</li> </ul>"}]}